<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="Programio" type="application/atom+xml"><link rel="icon" href="/images/favicon.png"><title>Architektura Druid.io - Programio</title><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro"><link rel="stylesheet" href="/css/main.css" type="text/css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--></head><body><header class="head"><h1 class="head-title u-fl"> <a href="/">Programio</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a href="/" class="head-nav__link">Domů</a></li><li class="head-nav__item"><a href="/archives" class="head-nav__link">Archiv</a></li><li class="head-nav__item"><a href="http://www.havrlant.cz/" class="head-nav__link">O autorovi</a></li><li class="head-nav__item"><a href="/rss.xml" class="head-nav__link">RSS</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time datetime="2015-05-11T16:42:56.000Z" class="post__time">11. 5. 2015</time><h1 class="post__title"><a href="/druid-io-architektura/">Architektura Druid.io</a></h1></header><div class="post__main echo"><p><a href="http://druid.io/" target="_blank" rel="external">Druid.io</a> je distribuovaná databáze napsaná v Javě, která se skládá z několika částí. Těmto částem říkáme <em>uzly</em> (<em>nodes</em>) a jsou to separátní Java procesy. Základní instalace Druidu potřebuje alespoň čtyři uzly: Realtime, Historický, Koordinátor a Broker, přičemž každý z nich má jinou zodpovědnost. V článku si všechny čtyři uzly představíme.</p>
<h2 id="dostáváme-data-do-druidu">Dostáváme data do Druidu</h2>
<p>Není typické, aby aplikace, které generují data (například naše Node.JS HTTP servery) posílaly data přímo do Druidu. Místo toho využívají nějakého messaging systému, do kterého zprávy posílají, a odtud se poté dostanou až do Druidu. My používáme <a href="/kafka/">Kafku</a>.</p>
<p>Začnu s popisem starší verze Druidu, kterou už sice nepoužíváme, ale jednodušeji se vysvětluje. Vstupním bodem celého Druid clusteru je <a href="http://druid.io/docs/0.7.0/Realtime.html" target="_blank" rel="external">Realtime uzel</a>.</p>
<h2 id="realtime-uzel">Realtime uzel</h2>
<p>Realtime uzel je služba, která se stará o načítání zpráv z Kafky nebo z jiného messaging systému. Spouští se jako obyčejný Java proces:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java io.druid.cli.Main server realtime</span><br></pre></td></tr></table></figure>
<p>V praxi by tam ale byla ještě hromada dalších parametrů, kterými se teď nebudeme zatěžovat. Realtime uzel potřebuje na vstupu ještě <a href="http://druid.io/docs/0.7.0/Realtime-ingestion.html" target="_blank" rel="external">konfigurační soubor</a>, tzv. “specFile”. My si z něj ukážeme jen pár částí. Začneme tou, kde se definuje, odkud se budou číst data:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">type</span>": <span class="value"><span class="string">"kafka-0.8"</span></span>,</span><br><span class="line">    "<span class="attribute">feed</span>": <span class="value"><span class="string">"impressions"</span></span>,</span><br><span class="line">    "<span class="attribute">parser</span>": <span class="value">&#123;</span><br><span class="line">    	"<span class="attribute">data</span>": <span class="value">&#123;</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"json"</span></span><br><span class="line">        </span>&#125;</span>,</span><br><span class="line">        "<span class="attribute">timestampSpec</span>": <span class="value">&#123;</span><br><span class="line">            "<span class="attribute">column</span>": <span class="value"><span class="string">"timestamp"</span></span>,</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"iso"</span></span><br><span class="line">        </span>&#125;        </span><br><span class="line">    </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>SpecFile říká, ať Realtime uzel čte zprávy z Kafka topicu <em>impressions</em> a ať očekává na vstupu JSON, který má vlastnost <em>timestamp</em>, ve kterém je uložen čas, kdy událost nastala. Toto je opravdu důležité – do Druidu lze dostat jen <em>události</em>, které nastaly v určitý čas a Druid potřebuje vědět kdy. Zprávy bez timestampu Druid zahodí.</p>
<!-- Tím řekneme Realtime uzlu, aby použil ovladače pro Kafka 0.8 a začal číst Kafka topic *impressions*. Vlastnost "data" říká, vstupní data očekáváme v JSON formátu. Vlastnost "timestampSpec" pak říká, že tyto JSONy budou mít v property "timestamp" časové razítko a to bude ve formátu ISO, tj. něco jako "2015-03-17T20:40:00Z". Schéma dat nikde nespecifikujeme, tj. nikde neříkáme, jaké property má JSON mít. 

Jen pozor na to, že když do topicu pošlete zprávu, která není JSON nebo nemá platné časové razítko v dané vlastnosti, tak Druid tuto zprávu **zahodí**. Když jsme Druida zkoušeli úplně poprvé, tak jsme tam prostě valili nějaké JSONy, které jsme měli po ruce, ale do Druidu se žádná data nedostala, protože neměla platné časové razítko. Validní vstup, který v našem případě reprezentuje zobrazení jednoho reklamního banneru, tj. impresi, by mohl vypadat třeba takto:


<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">timestamp</span>": <span class="value"><span class="string">"2015-03-17T20:40:00Z"</span></span>,</span><br><span class="line">    "<span class="attribute">campaignId</span>": <span class="value"><span class="string">"336"</span></span>,</span><br><span class="line">    "<span class="attribute">web</span>": <span class="value"><span class="string">"csfd.cz"</span></span>,</span><br><span class="line">    "<span class="attribute">browser</span>": <span class="value"><span class="string">"Chrome"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>–&gt;</p>
<p>Po spuštění začne uzel číst zprávy z topicu, zpracovávat je a ukládat je. V tuto chvíli jsou data dotazovatelná – mohli bychom poslat přes REST API dotaz na Realtime uzel a získat třeba počet zpracovaných impresí.</p>
<h2 id="segment">Segment</h2>
<p>Během čtení zpráv začne Realtime uzel vytvářet něco, čemu se říká <strong>segment</strong>. Segment je soubor, který obsahuje Druidem zpracovaná, indexovaná a agregovaná data za určitý čas. Zpracovaný segment už je dále neměnný, není ho možné updatovat.</p>
<p>Kolik dat bude v jednom segmentu obsaženo závisí na hodnotě segmentGranularity:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#34;granularitySpec&#34; : &#123;&#10;    &#34;type&#34; : &#34;uniform&#34;,&#10;    &#34;segmentGranularity&#34; : &#34;day&#34;,&#10;    &#34;queryGranularity&#34; : &#34;hour&#34;&#10;&#125;&#10;``` &#10;&#10;Druid by pro ka&#382;d&#253; nov&#253; den za&#269;al vytv&#225;&#345;et nov&#253; segment. Pro jednoduchost m&#367;&#382;eme nyn&#237; p&#345;edpokl&#225;dat, &#382;e m&#225;me-li denn&#237; segmenty, pak se pro ka&#382;d&#253; den vytvo&#345;&#237; jeden soubor, kter&#253; obsahuje v&#353;echna data z tohoto dne. Tento soubor se pak nakop&#237;ruje na n&#283;kter&#253; z Druid&#237;ch server&#367; a tento server zodpov&#237;d&#225; dotazy pro dan&#253; den.&#10;&#10;&#10;## &#381;ivotn&#237; cyklus segmentu&#10;&#10;Dejme tomu, &#382;e nastartujeme Realtime uzel v pond&#283;l&#237; v 11:15. Realtime uzel by za&#269;al okam&#382;it&#283; vytv&#225;&#345;et denn&#237; segment pro pond&#283;l&#237; a tento segment by zpracov&#225;val a&#382; do p&#367;lnoci. Po celou dobu by data za pond&#283;l&#237; byla na Realtime uzlu, ale byla by dotazovateln&#225; p&#345;es REST API. &#10;&#10;Dal&#353;&#237; zaj&#237;mav&#225; v&#283;c se za&#269;ne d&#237;t na konci dne, o p&#367;lnoci. Realtime uzel se za&#269;ne p&#345;ipravovat na dal&#353;&#237; segment. V 00:00 se vytvo&#345;&#237; dal&#353;&#237; segment a zpr&#225;vy z &#250;terka se zpracov&#225;vaj&#237; do tohoto nov&#233;ho &#250;tern&#237;ho segmentu.&#10;&#10;&#60;!---&#10;No jo, ale co kdy&#382; n&#225;m p&#345;ijde n&#283;jak&#225; zpr&#225;va pozd&#283;? Co kdy&#382; se n&#283;jak&#225; zpr&#225;va s &#269;asem 23:59:57 dostane k Druidu a&#382; v &#269;ase 00:01:12 dal&#353;&#237;ho dne? Druid umo&#382;&#328;uje nastavit &#269;as, po kter&#253; bude je&#353;t&#283; &#269;ekat na takto zpo&#382;d&#283;n&#233; zpr&#225;vy:</span><br></pre></td></tr></table></figure>
<p>“windowPeriod”: “PT10m” ```</p>
<p>Tímto bychom nastavili, že ještě deset minut po půlnoci bude čekat na zpožděné zprávy z předchozího dne. Zprávy s timestampem z pondělka by šly do pondělího segmentu, zprávy s timestampem z úterka do úterního. (Nezpomeňte, že každá zpráva si sama s sebou nese informaci o tom, kdy vznikla.)</p>
<p>Pondělní segment je pak považován za uzavřený a Druid už do něj žádnou další zprávu nepustí. Místo toho vytvoří <em>nový pondělní segment</em>. Dosud totiž Druid potřeboval mít pondělní data v takové struktuře, do které mohl snadno přidávat nově příchozí zprávy. Jenomže teď už je úterý a do pondělního segmentu žádná další data nepřijdou – proto Druid přechroustá všechny pondělní zprávy a vytvoří nové indexy. Celkově vytvoří novou strukturu, která bude optimalizována pro vyhledávání. –&gt;</p>
<p>Pondělní segment je v tuto chvíli považován za uzavřený a Druid už do něj žádnou další zprávu nevloží. Díky tomu může Druid pozměnit strukturu pondělního segmentu tak, aby data v něm byla optimalizována pro vyhledávání. Po uzavření pondělního segmentu proto Druid přeskládá všechny zprávy a vytvoří nové indexy, aby se v segmentu dalo rychle vyhledávat.</p>
<p>Tento upravený segment odešle do <em>deep storage</em>. Tím Druiďáci nazývají vzdálené úložiště, ve kterém jsou archivovány všechny segmenty. V současné době je podporováno <a href="http://hortonworks.com/hadoop/hdfs/">HDFS</a> a <a href="http://aws.amazon.com/s3/">Amazon S3</a>. Tohle je podstatná informace – Druid neobsahuje nic jako trvalé úložiště pro zpracované segmenty. Samotná data musíme uložit jinam. Na druhou stranu napojení na ostatní služby není nijak těžké. My používáme HDFS.</p>
<p>Metadata o novém segmentu uloží Druid do MySQL. Základní schéma vypadá takto:</p>
<div class="figure">
<img src="/images/druid/deepstorage.svg" alt="" />

</div>
<!--
## MySQL
 
Nyní se podíváme na to, jak spolu jednotlivé části Druidu komunikují. Protože Realtime uzel odeslal segment do deep storage, musí o tom dát vědět zbytku clusteru. To se děje pomocí dalších dvou externích závislostí: MySQL a ZooKeeper. Po vytvoření segmentu by Realtime uzel uložil metadata o segmentu do MySQL. Část dat z naší MySQL: 


| dataSource  | start                    | end                      | created_date             |
|-------------|--------------------------|--------------------------|--------------------------|
| ssp-auction | 2015-03-13T11:00:00.000Z | 2015-03-13T12:00:00.000Z | 2015-03-13T12:53:25.540Z |
| ssp-auction | 2015-03-13T12:00:00.000Z | 2015-03-13T13:00:00.000Z | 2015-03-13T14:04:42.668Z |
| ssp-auction | 2015-03-13T13:00:00.000Z | 2015-03-13T14:00:00.000Z | 2015-03-13T15:16:01.205Z |
| ssp-auction | 2015-03-13T14:00:00.000Z | 2015-03-13T15:00:00.000Z | 2015-03-13T16:10:22.088Z |
| ssp-auction | 2015-03-13T15:00:00.000Z | 2015-03-13T16:00:00.000Z | 2015-03-13T16:51:05.289Z |


Vidíme, že v databázi jsou uložené informace jako je čas vzniku segmentu a který interval segment obsluhuje. Kromě těch sloupečků je v databázi ještě některé další sloupečky jako `version` nebo `used`. Z databáze si můžeme všimnout, že vytvoření segmentu není úplně nenáročná činnost: V prvním řádku máme segment, který má data pro interval 11:00--12:00, přitom tento segment vznikl až v 12:53. Window period jsme měli na 15 minutách, takže Druidu trvalo nějakých 38 minut, než segment vytvořil, tj. než zkonvertoval původní strukturu do immutable struktury. 

V tuto chvíli je segment na dvou místech: na Realtime uzel a v deep storage. Protože Realtime uzel už začíná zpracovávat další segment, chce se toho předchozího zbavit. Pokud bychom v tuto chvíli totiž položili dotaz, který by dotazoval data z pondělí, stále by na to odpovídal Realtime uzel. 
-->
<h2 id="koordinátor-a-historický-uzel">Koordinátor a Historický uzel</h2>
<p>Nyní přichází na řadu další dva druidí uzly: <a href="http://druid.io/docs/0.7.0/Coordinator.html" target="_blank" rel="external">Koordinátor</a> a <a href="http://druid.io/docs/0.7.0/Historical.html" target="_blank" rel="external">Historický</a>. Oba uzly jsou zase jen další Java procesy spuštěné na serverech. Historický uzel nezpracovává žádná nová data, jen <strong>přebírá segmenty, které vytvořil Realtime uzel.</strong></p>
<p>Koordinátor řídí přiřazování segmentů. Máme-li v clusteru deset Historických uzlů, musíme se rozhodnout, který z nich bude servírovat náš pondělní segment. Koordinátor je ten, kdo má v tomto konečné slovo. Proto Koordinátor sleduje stav clusteru a když zjistí, že přibyl nový segment, nalezne vhodný Historický uzel a přikáže mu, aby si nový segment stáhl z deep storage. Segmenty se nikdy nepřesouvají mezi Druidími uzly přímo, vždy jen přes deep storage.</p>
<p>Pomůžeme si dalším obrázkem:</p>
<div class="figure">
<img src="/images/druid/druidarch.svg" alt="">

</div>
<p>Když Historický uzel stáhne segment, oznámí to clusteru a všechny dotazy na data, která jsou uložena v tomto segmentu, půjdou na tento server. Za normálních okolností platí, že všechny dosud vytvořené segmenty jsou buď na Historických uzlech, nebo na Realtime uzlech.</p>
<p>Časová posloupnost akcí při vytváření segmentu by mohla vypadat takto:</p>
<ul>
<li>úterý 00:00: Realtime uzel začne vytvářet úterní segment. Oznámí clusteru, že všechny dotazy na úterní data mají jít k němu. Dále uzavírá pondělní segment a začíná vytvářet nové indexy pro pondělní segment.</li>
<li>01:30 Kompilace pondělního segmentu dokončena, Realtime uzel začíná nahrávat segment do HDFS.</li>
<li>01:50 Nahrávání dokončeno, Realtime oznamuje clusteru, že vytvořil nový segment.</li>
<li>01:51 Koordinátor přikáže některému Historickému uzlu, aby si k sobě stáhnul pondělní segment.</li>
<li>02:08 Historický uzel dokončil stahování segmentu a informuje cluster o tom, že bude odpovídat na všechny dotazy za pondělek.</li>
<li>02:09 Realtime uzel maže pondělní segment a oznamuje clusteru, že už dále nebude odpovídat na dotazy na pondělní data.</li>
</ul>
<p>Teď ale máme data za pondělí na Historickém uzlu a data za úterý na Realtime uzlu. Co když chceme znát součet impresí za oba dny současně? Musíme se zeptat obou serverů. Naštěstí to ale nemusíme dělat ručně, Druid nám poskytuje čtvrtý typ uzlu, který právě řeší dotazování.</p>
<h2 id="broker-uzel">Broker uzel</h2>
<p>Všechny dotazy posíláme <a href="http://druid.io/docs/0.7.0/Broker.html" target="_blank" rel="external">Brokeru</a>. Broker je další uzel Druidu, který za nás řeší dvě věci:</p>
<ul>
<li>Ptáme-li se na nějaká data, Broker zjistí, které segmenty tato data obsahují a které uzly tyto segmenty servírují.</li>
<li>Broker přepošle dotaz všem uzlům, které servírují potřebné segmenty a čeká na odpovědi. Tyto odpovědi následně spojí do jedné odpovědi (například sečte počty impresí za pondělí a za úterý) a odešle tuto odpověď zpátky klientovi.</li>
</ul>
<p>Broker uzel nám jako klientům zajišťuje transparentnost – nemusí nás zajímat, na kterých serverech jsou uložena naše data, jestli je servíruje Realtime uzel nebo Historický uzel. Broker dále cachuje výsledky z Historických uzlů, čímž se výrazně zlepšuje výkonnost celého clusteru. Historické uzly obsahují immutable segmenty, takže pro dva stejné dotazy vrátí vždy stejné výsledky. Cachovat data z Realtime uzlu si nemůže dovolit, protože tam se data typicky mění každou sekundu.</p>
<div class="figure">
<img src="/images/druid/druidarch2.svg" alt="">

</div>
<h2 id="jak-výpadek-jedné-ze-služeb-ovlivní-chod-clusteru">Jak výpadek jedné ze služeb ovlivní chod clusteru</h2>
<h3 id="výpadky-realtime-uzlu">Výpadky Realtime uzlu</h3>
<p>Na Realtime uzel jsou kladeny největší nároky – musí zpracovávat příchozí zprávy, zároveň je musí poskytovat k dotazování a k tomu všemu musí být nějak odolný vůči výpadkům.</p>
<ul>
<li><p>První přístup byl takový, že si Realtime uzel průběžně ukládal všechna data, která zpracovával, na disk. Po pádu Realtime uzlu a jeho restartu si mohl načíst již zpracovaná data z disku a mohl pokračovat dále tam, kde přestal. Toto ale zdaleka není stoprocentní řešení – mohlo se stát, že se nějaké zprávy zduplikují, ale to zase tak moc nevadí, protože <a href="/lambda-architecture">lambda architektura</a>. Jenomže pokud by shořel celý server, na kterém Realtime uzel běží, tak bychom přišli o všechna data. Navíc ve chvíli, kdy neběží Realtime uzel, třeba i jen minutu, nemáme přístup k datům, které už zpracoval.</p></li>
<li><p>Proto se začaly ručně vytvářet repliky Realtime uzlů. To se bohužel muselo trochu nahackovat přes Kafka consumer group ID – spustily se dva Realtime uzly a každému se nastavilo jiné group ID, viz <a href="/kafka-consumer/">Jak funguje Kafka consumer</a>. Jenomže to způsobilo <a href="https://groups.google.com/forum/#!searchin/druid-development/realtime$20uzel$20kafka$20replication/druid-development/LRFr2Dzw5Po/wAnCtm4FmrAJ" target="_blank" rel="external">další problémy</a> a celkově je to víc hack než řešení. Navíc už není jednoduché jen tak přivézt k životu jednou spadlý Realtime uzel. Máme-li dva Realtime uzly, po pádu jednoho z nich a po jeho znovu nastartování získáme nekonzistentní stav clusteru, protože uzel, který spadl, obsahuje třeba hodinu stará data, zatímco uzel, který celou dobu běžel, má aktuální data.</p></li>
<li><p>To se nakonec vyřešilo tím, že se <a href="https://groups.google.com/forum/#!searchin/druid-development/realtime$20uzel$20kafka$20replica/druid-development/aRMmNHQGdhI/HIB9kRf_6CwJ" target="_blank" rel="external">přešlo od pull-based řešení k push-based řešení</a>. V novějších verzích se proto typicky data do Druidu posílají, místo toho, aby si je Druid sám tahal z Kafky. O tom se ale budeme víc bavit někdy příště. Tento přístup zajistil, že replikace je už snadno konfigurovatelná věc. Můžeme nastavit, že chceme, aby se Realtime uzel třikrát replikoval. Spadne-li jedna replika, automaticky se obnoví až s dalším segmentem – nenastává problém s tím, že bychom získali hodinu stará data.</p></li>
</ul>
<h3 id="výpadky-zbylých-částí-clusteru">Výpadky zbylých částí clusteru</h3>
<ul>
<li><p>Když vypadne jeden <strong>Broker</strong>, můžeme se zeptat druhého.</p></li>
<li><p>Když vypadne <strong>Historický uzel</strong>, převezme jeho práci jiný Historický uzel. V Druidu můžeme nastavit replikaci Historických uzlů, takže pokud ji nastavíme na 2, po výpadku jednoho Historického uzlu se nestane nic – v clusteru bude vždy existovat ještě jeden uzel, který obsahuje dané segmenty. Pokud by vypadly všechny repliky pro daný segment, musel by se segment časem stáhnout na jiný Historický uzel. Do té doby, než by se segment stáhl, by byla data nedotazovatelná, ale zbytek funkčnosti clusteru by to neovlivnilo.</p></li>
<li><p>Když vypadne <strong>Koordinátor</strong> a neexistuje jeho replika, tak se zmrazí stav clusteru. V tuto chvíli není možné, aby se segment z Realtime uzlu dostal na Historický uzel, protože Historickému uzlu nemá kdo říci, aby si stáhl nový segment. Nicméně segment, který zůstal na Realtime uzlu je nadále dotazovatelný. Problém by nastal až ve chvíli, kdy by na Realtime uzlu docházelo místo nebo paměť.</p></li>
<li><p>Když vypadne <strong>MySQL</strong>, tak je to podobné, jako když vypadne Koordinátor.</p></li>
</ul>
<h2 id="horizontální-škálovatelnost">Horizontální škálovatelnost</h2>
<h3 id="realtime-uzel-1">Realtime uzel</h3>
<p>Ve starém přístupu jsme mohli vytvořit tolik <strong>Realtime uzlů</strong>, kolik partitionů obsahoval náš Kafka topic. V novém push-based přístupu už je to zcela nezávislé a můžeme si sami nastavit, kolik Realtime uzlů má zpracovávat vstupní zprávy. Můžeme nastavit, že naše zprávy budou zpracovávány třemi shardy a replikaci můžeme nastavit na dva. Tím dosáhneme toho, že se automaticky vytvoří celkem šest Realtime uzlů, které bude zpracovávat naše data. Vstupní zprávy se rozdělí na třetiny a každá třetina je poslána jiné dvojice uzlů.</p>
<p>Pro příklad předpokládejme, že máme devět zpráv, očíslované od jedné do devíti. Každý ze tří shardů bude zpracovávat “každou třetí zprávu”:</p>
<div class="figure">
<img src="/images/druid/DruidReplikace.svg" alt="">

</div>
<p>Potřebujeme-li se dotázat na aktuální data, musíme se vždy zeptat tří Realtime uzlů. Vypadne-li Realtime #3, zastoupí ho #4. Vypadne-li i ten, jsou zprávy ze Shardu #2 nedostupné. Realtime uzly #3 a #4 by se automaticky znova nastartovaly s novým segmentem (tj. v našem příkladě se začátkem nového dne).</p>
<h3 id="ostatní-uzly">Ostatní uzly</h3>
<ul>
<li><p><strong>Koordinátor</strong> horizontálně škálovatelný není. Máme-li více Koordinátorů, zvolí se mezi nimi leader a ten koordinuje cluster. Koordinátor nicméně vykonává nenáročnou činnost, jeden aktivní uzel by měl bohatě stačit.</p></li>
<li><p><strong>Historické uzly</strong> jsou sobě zcela nezávislé a můžeme jich mít kolik chceme. Každý Historický uzel zkrátka servíruje určitý počet segmentů, které jsou navíc immutable, a Broker ví o tom, který segment je uložený na kterém Historickém serveru. Můžeme mít klidně pro každý segment, tj. pro každá denní data, vlastní Historický uzel.</p></li>
<li><p><strong>Broker</strong> je opět nezávislý uzel, který nepotřebuje vědět o ostatních Brokerech. Broker jen přijímá dotazy a rozesílá je na Historické a Realtime uzly. Můžeme jich mít kolik chceme a v aplikaci pak střídavě rozesílat dotazy na různé Brokery.</p></li>
</ul>
<p>Celkově vzato je Druid velmi horizontálně i vertikálně škálovatelný. Můžeme proto postavit větší cluster, než jaký jsme viděli na posledním obrázku. Pokud bychom nechali počet Realtime uzlů a jen zvětšili počet Historických uzlů, mohl by dotaz „vrať mi součet impresí za uplynulý týden“ položený v neděli dopadnout takto:</p>
<div class="figure">
<img src="/images/druid/DruidCluster.svg" alt="">

</div>
<p>Broker se za nás zeptá Realtime uzlu z každého shardu a zároveň se zeptá všech Historických uzlů, které zrovna obsahují data, která potřebujeme. Počká na všechny odpovědi, ty spojí a vrátí uživateli.</p>
<p>A jaký je <a href="https://groups.google.com/forum/#!topic/druid-user/xPdSmowsQD4" target="_blank" rel="external">dosud největší Druid cluster</a>?</p>
<ul>
<li>50–100 PB surových dat, z kterých Druid po všech kompresích vytvořil 500 TB dat.</li>
<li>20 bilionů událostí (~ řádků).</li>
<li>400 serverů.</li>
</ul>
<h2 id="konzistence-dat">Konzistence dat</h2>
<p><strong>Data nejsou konzistentní napříč Realtime uzly.</strong> Může se totiž stát, že Realtime uzel #1 zpracovává zprávy s offsetem 10 000, zatímco Realtime uzel #2, jeho replika, je trochu pozadu a zpracovává zprávy teprve s offsetem 9 750, tedy je o 250 zpráv pozadu. Tomu se nedá vyhnout, ať už používáme starý přístup, nebo nový přístup. Broker se přitom může jednou zeptat Realtime uzlu #1 a potom uzlu #2. Potom by se mohlo stát, že by v prvním dotazu zjistil od uzlu #1, že už proběhlo 10 000 impresí, ale o sekundu později by z uzlu #2 zjistil, že proběhlo 9 750 impresí, což je zjevně nesmysl, aby o sekundu později nastalo <em>méně impresí</em>.</p>
<p>Nicméně pokud oba Realtime uzly stíhají, tak by tato nekonzistence měla být nepostřehnutelná.</p>
<p>Data napříč <strong>Historickými uzly jsou plně konzistentní</strong>, protože pokud dva Historické uzly servírují segment pro daný časový úsek, tak tyto dva segmenty jsou zcela identické.</p>
<!--
## Více segmentů pro jeden den

Prozatím jsme uvažovali, že když máme segmentGranularity nastavenou na jeden den, že za den vznikne jeden segment = jeden soubor. Ve skutečnosti těch souborů vznikne více. Když máme v běhu dva Realtime uzly, které zpracovávají vždy polovinu dat, tak každý Realtime uzel si vytváří svůj vlastní segment. Tedy každý den by každý Realtime uzel vyprodukoval svůj vlastní segment a tyto segmenty už by se nikde nespojovaly do jednoho velkého segmentu. Je proto možné mít pro jeden den více segmentů.
-->
<h2 id="zookeeper">ZooKeeper</h2>
<p>V předchozích obrázcích jsem například nakreslil šipku z Koordinátoru přímo do Historického uzlu. To nebylo úplně přesné, protože ve skutečnosti Koordinátor s historickým uzlem nekomunikuj přímo, ale pomocí <a href="https://zookeeper.apache.org/" target="_blank" rel="external">ZooKeeperu</a>.</p>
<p>ZooKeeper je hojně používaná služba v distribuovaných systémech, protože umožňuje takové věci jako zvolit leadera v distribuované síti, udržuje synchronizovaný log napříč svými servery a často se používá na ukládání konfigurací či jiných metadat. Kromě toho poskytuje klienty, které jsou schopny reagovat na změnu dat v Zookeeperu.</p>
<p>Potřebuje-li Koordinátor něco říct Historickému uzlu, uloží na předem domluvené místo v ZooKeeperu danou zprávu a Historický uzel si potom sám všimne, že se v ZooKeeperu na dané adrese něco změnilo, zjistí co a provede příslušnou akci.</p>
<p>Ve skutečnosti tak obrázek s komunikací vypadá takhle: (vypůjčeno přímo z <a href="http://druid.io/docs/0.7.0/Design.html" target="_blank" rel="external">dokumentace Druidu</a>)</p>
<div class="figure">
<img src="/images/druid/druid-manage-1.png" alt="">

</div>
<p>Všechny šipky jdou vždy z druidího uzlu do ZooKeeperu, nikdy ne do dalšího druidího uzlu. Na principu se nic nezměnilo. Druiďáci ale mají v plánu <a href="https://groups.google.com/forum/#!topic/druid-development/5s0Uh2NJ15c" target="_blank" rel="external">použití Zookeeperu omezit</a>. Což není úplně neobvyklé, nová Kafka už také daleko méně závisí na ZooKeeperu než starší verze.</p>
<p>Přes ZooKeeper také Realtime a Historické uzly sdělují celému clusteru jaké segmenty jsou u nich aktuálně uložené. Na tyto informace pak reaguje Broker, který musí v každé chvíli vědět, koho se má zrovna zeptat.</p>
<p>Příště si ukážeme, jak Druid agreguje a zpracovává všechny vstupní zprávy, ze kterých vytváří segmenty.</p>
<p>A jestli vás článek opravdu zaujal, <a href="http://www.ibillboard.com/cs/spolecnost/spolecnost/kariera/202-vyvojar-nodejs-javascript">tak pojďte pracovat k nám do firmy!</a> Podobných problémů řešíme mraky.</p><div class="relatedBox"><div class="relatedArticlesBox"><h2 id="relatedArticles">Související články:</h2><ol><li><a href="/lambda-architecture/">Jak zpracováváme velké množství dat: Lambda Architecture</a></li><li><a href="/kafka/">Kafka messaging system</a></li><li><a href="/kafka-consumer/">Jak funguje Kafka consumer</a></li><li><a href="/kafka-replikace/">Jak funguje replikace v Kafce</a></li><li><a href="/samza/">Samza: distributed stream processing framework</a></li><li><a href="/samza-windowing/">Windowing v Samze</a></li><li><a href="/samza-local-state/">Uložení lokálního stavu v Samze</a></li><li><a href="/druid-io/">Druid.io: distribuovaná immutable databáze pro analytické výpočty</a></li><li><a href="/druid-io-ingest/">Jak Druid.io agreguje data</a></li></ol></div></div><div class="articleTags"><strong><a href="/tags/bigdata">#bigdata</a></strong>, <strong><a href="/tags/druid">#druid</a></strong></div></div></article><div style="text-align: right"><div class="fb-like" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true" style="margin-top: 10px; height: 30px"></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/bigdata/" class="post__tag__link">bigdata</a></li><li class="post__tag__item"><a href="/tags/druid/" class="post__tag__link">druid</a></li></ul><a href="/druid-io-architektura/#disqus_thread" class="post__foot-link u-fr">Komentáře</a></footer></main><footer class="foot"><div class="foot-copy u-fl"><a href="/">Programio</a> píše <a href="http://www.havrlant.cz/">Lukáš Havrlant</a></div><menu class="page-menu u-fr"><li class="page-menu__item"><a title="Previous" href="/druid-io-ingest/" class="page-menu__link icon-arrow-left"></a></li><li class="page-menu__item"><a title="Next" href="/druid-io/" class="page-menu__link icon-arrow-right"></a></li></menu></footer><script>(function(h,g,l,k,j,i){j=h.createElement(g),i=h.getElementsByTagName(g)[0],
j.src="//"+l+".disqus.com/"+k+".js",i.parentNode.insertBefore(j,i)})
(document,"script","programio","embed");
</script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=
function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;
e=o.createElement(i);r=o.getElementsByTagName(i)[0];
e.src='//www.google-analytics.com/analytics.js';
r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','UA-1559810-13');ga('send','pageview');
</script><div id="fb-root"></div><script>(function(d, s, id) {
var js, fjs = d.getElementsByTagName(s)[0];
if (d.getElementById(id)) return;
js = d.createElement(s); js.id = id;
js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&appId=166268190106534&version=v2.0";
fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script></body></html>