<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title><![CDATA[Programio]]></title>
    <link>http://programio.havrlant.cz/</link>
    <atom:link href="/rss.xml" rel="self" type="application/rss+xml"/>
    <description></description>
    <pubDate>Thu, 28 Jan 2016 18:31:03 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title><![CDATA[Antipatterny Event sourcingu]]></title>
      <link>http://programio.havrlant.cz/event-sourcing-antipatterny/</link>
      <guid>http://programio.havrlant.cz/event-sourcing-antipatterny/</guid>
      <pubDate>Sun, 13 Dec 2015 07:07:19 GMT</pubDate>
      <description>
      <![CDATA[<p>V <a href="/architektura-event-sourcing">minulém článku jsem představil event sourcing</a> a popsal, jak ho u nás v práci používáme. Dnes]]>
      </description>
      <content:encoded><![CDATA[<p>V <a href="/architektura-event-sourcing">minulém článku jsem představil event sourcing</a> a popsal, jak ho u nás v práci používáme. Dnes se podíváme na to, jaké problémy nám přechod na event sourcing přinesl a jak jsme je vyřešili a co jsme si tak nějak označili jako anti-patterny, kterým je lepší se vyhnout.</p>
<h2 id="neidempotence">Neidempotence</h2>
<p>Od Událostí očekáváme, že jejich aplikace bude idempotentní. Idempotence znamená, že</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#8704;x&#8712;D(f): f(x)=f(f(x))</span><br></pre></td></tr></table></figure>
<p>Méně srozumitelně: pokud stejnou Událost aplikujeme na entitu vícekrát, nesmí to změnit stav entity. Nejlépe se to vysvětluje na příkladu: “zvyš mzdu programátora o deset tisíc” není idempotentí Událost, “nastav plat programátora na sto tisíc” je idempotentí Událost. Pokud první Událost aplikujeme pětkrát, zvýšili jsme mzdu programátora o padesát tisíc. U druhého typu Událostí to nehrozí.</p>
<p>Smyslem tohoto pravidla je umožnit zpracovat například posledních sto Událostí znova bez toho, aniž by to rozbilo celý systém a také se to lépe debuguje – zkrátka kouknete na Událost a hned víte, co uživatel nastavil a jaký je aktuální stav systému.</p>
<p>Stejně tak bychom nikdy neměli nic vyvozovat z počtu daných Událostí. Pokud bychom například chtěli uživateli zobrazovat, kolikrát se už pokusil změnit název Kampaně, nemůžeme to udělat tak, že po každé, když se zavolá metoda <code>applyCampaignNameSet</code> inkrementujeme proměnnou, protože se teoreticky mohlo stát, že se metoda <code>applyCampaignNameSet</code> zavolala dvakrát se stejnou Událostí a už bychom to zobrazovali špatně.</p>
<p>Místo toho tak musíme v takovém případě mít informaci o počtu Událostí přímo v datech dané Události. Vyprodukujeme-li třetí Událost <code>CampaignNameSet</code> pro danou Kampaň, uložíme přímo do Události, že je třetí v pořadí. Všechny View Buildery si pak mohou informaci o počtu Událostí daného typu přečíst přímo z Události.</p>
<h2 id="neznámé-události">Neznámé Události</h2>
<p>Občas se nám stalo, že jsme releasli novou verzi aplikace, která vyprodukovala Událost, kterou ostatní aplikace neznaly. Třeba jsme přidali kód, který automaticky hlídal konce kampaně a když tento konec nastal, vypálila se Událost <code>CampaignStateChanged</code>. Kvůli nějaké chybě se stalo, že tenhle kód šel do světa dříve než aktualizace View Builderů, takže když se potom o půlnoci vyprodukovaly první Události <code>CampaignStateChanged</code>, které některým Kampaním nastavily stav na “Finished”, celý zbytek systému byl těmito Událostmi zmaten, protože tuto Události neznal. Nabízí se v zásadě dvě řešení:</p>
<ul>
<li><p>Neznámé Události přeskočit. S tím je ten trabl, že ty překočené Události už se nikdy znova nezpracují. Jako by neexistovaly – pokud ručně neresetujeme View Builder, aby zpracoval všechny Události znova. Do té doby, než bychom resetovaly View Builder, by se tak daná Kampaň jevila jako neukončená celému systému, což jistě není správně.</p></li>
<li><p>Druhou možností je, že když aplikace narazí na Událost, kterou nezná, tak spadne nebo přinejmenším přestane číst další Události. To taky není úplně šťastné řešení. Představme si, že ta nová neznámá Událost je třeba <code>CampaignNameSet</code>. Takovému Jádru, které vybírá, jakou reklamu uživateli zobrazit, je název Kampaně úplně šumák, takže by bylo smutné, kdybychom kvůli této neznámé Události přestali číst další Události. Třeba hned v další Události někdo chtěl snížit cenu za proklik – a my bychom tuto změnu neaplikovali jenom proto, že se uživatel snažil přejmenovat Kampaň a Jádro nevědělo co s tím.</p></li>
</ul>
<p>Celá věc se komplikuje tím, že chceme být obecně schopni releasovat kdykoliv jakoukoliv část systému bez toho, aniž by to ohrozilo ostatní. Zatím je nicméně naše řešení takové, že aplikace spadne nebo se zastaví čtení dalších Událostí, pokud natrefí na Událost, kterou nezná a programátoři musí daný problém rychle vyřešit.</p>
<h2 id="propagace-událostí">Propagace Událostí</h2>
<p>Mějme Stránku, která má pět Sekcí, které zase mají ještě pět Sekcí. To je celkem třicet Sekcí pod jednou hlavní Stránkou. A teď co se má stát, když uživatel nastaví Stránce, že se tam nesmí zobrazovat pornografický obsah? Toto nastavení se typicky dědí i do sekcí. My samozřejmě vypálíme Událost <code>SiteRestrictionSet</code>, otázkou ale je, jestli máme také vypálit třicet Událostí <code>SectionRestrictionSet</code> pro každou Sekci zvlášť.</p>
<ul>
<li><p>Když to neuděláme a necháme jen tu jednu Událost <code>SiteRestrictionSet</code>, tak všechny aplikace, které reagují na tuto Událost, budou muset po svém rešit logiku dědění, tj. budou muset řešit problém “Já sice nemám nastaveou žádnou restrikci, ale můj prapředek ano”. Tuto logiku bychom museli více méně rozkopírovat všude, kde se vytváří nějaký model se Stránkami.</p></li>
<li><p>Když to uděláme a všem Sekcím řekneme pomocí Událost <code>SectionRestrictionSet</code>, že se jim změnilo nastavení, nemusíme už nikde nic zvláštního doprogramovávat, protože příslušné <code>applySectionRestrictionSet</code> metody už všude máme. Na druhou stranu to znamená, že místo jedné Události jich budeme mít třicet. Přitom existují vlastnosti, které se budou měnit daleko častěji a dynamičtěji než zrovna nastavení restrikcí.</p>
<ul>
<li>Předchozí problém s hromadou Událostí by teoreticky šel řešit tak, že dovolíme, aby jednu stejnou Událost mohlo zpracovat více entit. Zatím to máme tak, že jedna Událost patří vždy k jedné entitě. Mohli bychom upravit náš event framework tak, abychom mohli říct, že jedna Událost <code>SectionRestrictionSet</code> může “patřit” několika entitám zároveň.</li>
</ul></li>
</ul>
<p>Zatím nemáme jednotný způsob jak podobné případy řešit, někdy to vyřešíme propagací Událostí, někdy ne. Postupem času se ale asi více přikláníme k první možnosti a Události spíše propagujeme. Dá se to lépe zautomatizovat a množství Událostí nám zatím takový problém nedělá. Ale možná jednou začne.</p>
<p>Máte s Event sourcingem více zkušeností? Jak podobné problémy řešíte vy?</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/event-sourcing-antipatterny/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Jak vypadá architektura našeho systému: Event sourcing a CQRS]]></title>
      <link>http://programio.havrlant.cz/architektura-event-sourcing/</link>
      <guid>http://programio.havrlant.cz/architektura-event-sourcing/</guid>
      <pubDate>Mon, 16 Nov 2015 17:07:19 GMT</pubDate>
      <description>
      <![CDATA[<p>Aktuálně vyvíjíme nový video ad server. Plus minus je to systém, pomocí kterého můžete zadávat reklamní kampaně, které se pak budou zobra]]>
      </description>
      <content:encoded><![CDATA[<p>Aktuálně vyvíjíme nový video ad server. Plus minus je to systém, pomocí kterého můžete zadávat reklamní kampaně, které se pak budou zobrazovat ve video přehrávačích. V článku popíši, jaké technologie jsme pro to použili a jak jsme je všechny zkombinovali do jednoho funkčního celku.</p>
<p>Celý příběh začíná ve webovém UI/administraci, což je single-page aplikace napsaná v Angularu (promiň, Dane). Web reaguje s backendem přes AJAX na základě <a href="http://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">vzoru CQRS</a> a backend samotný je postavený na základech <a href="http://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="external">Event Sourcingu</a>.</p>
<h2 id="commandy-a-queries">Commandy a Queries</h2>
<p>Veškerá interakce mezi webovým UI a backendem probíhá skrze Commandy a Queries:</p>
<ul>
<li><strong>Query</strong> je ajaxový HTTP GET požadavek, který UI vypálí, když potřebuje zjistit nějaké informace z backendu. Query nikdy nesmí změnit stav systému, jen vrátit data. Typická Query může být <em>Vrať mi seznam kampaní daného uživatele</em>.</li>
<li><strong>Command</strong> je HTTP POST (PUT…) požadavek, který se pokouší změnit stav systému. Typický Command je <em>Vytvoř novou kampaň</em> nebo <em>změň název kampaně</em>. Commandy nikdy nevrací žádná data, v odpovědi máme jen jednoduchý indikátor toho, jestli se Command podařilo zpracovat, nebo jestli nastala chyba.</li>
</ul>
<p>Některé důsledky předchozích dvou bodů:</p>
<h3 id="optimistic-ui">Optimistic UI</h3>
<p>Pokud vypálíme nějaký Command, předpokládáme, že uspěje. Přejmenuje-li uživatel kampaň, my vypálíme správný Command, ale v UI už všude ukazujeme nové jméno bez ohledu na to, jestli Command na backendu prošel, nebo neprošel. Říká se tomu <em>Optimistic UI updating</em>. Výsledkem je, že UI reaguje okamžitě na akce uživatele, rychlost odezvy backendu na to nemá vliv.</p>
<p>Pokud ale Command na backendu opravdu selže, musíme nějak zareagovat v UI, nemůžeme se celou dobu tvářit, že se název kampaně změnil, i když se ve skutečnosti na backendu nezměnil. Tohle je obecně těžký problém, protože to selhání může nastat třeba až za deset sekund a během té doby už uživatel mohl přejít na jinou stránku a mohl udělat několik dalších akcí. Nemáme to dobře vyřešené pro všechny případy a ještě s tím asi budeme bojovat.</p>
<p>Protože nemáme úplně ideálně vyřešené undo akce Commandů, snažíme se alespoň provádět co nejvíce validací přímo v UI, abychom na backend nepálili Commandy, které jistě neprojdou byznys logikou na backendu. Má-li být zadaná cena kladná, nedovolíme odeslat Command se zápornou cenou. Samozřejmě to ale nikdy nebude stoprocentní, když už nic, tak backend může spadnout.</p>
<h3 id="performance">Performance</h3>
<p>Protože Commandy nevrací žádná data a předpokládáme, že projdou, není potřeba, aby jejich zpracování bylo extrémně rychlé. To má za následek, že si při jejich zpracování můžeme dovolit provádět více činností a celkově nemusí být celý kód kolem zpracování Commandů optimalizovný na rychlost. Což je v naprostém protikladu se zpracováním Queries, na které naopak uživatel čeká. Query posíláme, když potřebujeme z backendu zjistit nějaké informace, proto se naopak snažíme, aby zpracování Queries bylo co nejrychlejší.</p>
<h3 id="generování-idéček">Generování IDéček</h3>
<p>Zajímavá situace nastane, když v UI vytváříme novou kampaň. Člověk by asi čekal, že když pošlu požadavek na vytvoření nové kampaně, backend mi v odpovědi vrátí ID této kampaně, abych ji mohl dále adresovat. Jenže naše Commandy nevrací žádná data. Vyřešili jsme to tak, že ID nově vzniklé kampaně se už posílá v samotném Commandu. V rámci POST požadavku na vytvořením nové kampaně pošleme i její ID. Používáme <a href="https://github.com/broofa/node-uuid" target="_blank" rel="external">UUID v4</a>, takže máme prakticky garantované, že toto vygenerované ID bude unikátní. Webové UI zkrátka vygeneruje nové UUID a pošle ho backendu, který případně může zkontrolovat, jestli už dané ID není použito.</p>
<h2 id="event-sourcing">Event sourcing</h2>
<p>Další buzzword, který máme na skladě, je <strong>event sourcing</strong>. Základní myšlenkou event sourcingu je, že hlavní databáze obsahuje historii všech změn, které uživatel v systému provedl a jedině od těchto změn se odvíjí stav systému. Historie změn = Single Source of Truth. Historie změn je už z principu neměnná, co se jednou stalo, nemůže se odestát. Přečte-li nějaká aplikace všechny změny, dostane aktuální stav systému. Těmto <em>změnám</em> pak říkáme <strong>Eventy</strong> neboli <strong>Události</strong>.</p>
<p>V praxi to u nás funguje takto: uživatel změní v administraci název kampaně, my vypálíme Command na backend, tam ověříme nějakou byznys logiku (dejme tomu jestli název není moc dlouhý) a pokud je vše v pořádku, vytvoříme Událost <code>CampaignNameSet</code>. Tuto Událost uložíme do <em>Event Store</em>, což máme aktuálně implementované jako kolekci v <a href="https://www.mongodb.org/" target="_blank" rel="external">Mongu</a> (v SQL světě by to byla prostě tabulka). V této kolekci máme uchované všechny Události, které kdy v našem systému nastaly. Událost je přitom jednoduchý JSON objekt, něco takového:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    eventType: "CampaignNameSet",</span><br><span class="line">    entityId: "cf3f128e-5051-47fe-a961-da3e55422258",</span><br><span class="line">    datetime: "2015-09-27T06:26:51.312Z",</span><br><span class="line">    data: &#123;</span><br><span class="line">        name: "Nový název kampaně"</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Těch dat je tam ve skutečností více, ale to je teď jedno. A proč používáme zrovna Mongo? Potřebovali jsme databázi, která obstojně zvládá replikace a shardování a umí dobře pracovat s JSONem. Mongo to tehdy umělo asi nejlépe.</p>
<h2 id="instancování-entit-při-zpracování-commandů">Instancování entit při zpracování Commandů</h2>
<p>Při zpracování Commandu potřebujeme znát aktuální stav systému, abychom mohli vyhodnotit všechna pravidla. Při zpracování Commandu na přejmenování kampaně bychom mohli kontrolovat dvě pravidla: jestli není název moc dlouhý a jestli se liší od předchozího názvu – nemá smysl přejmenovávat kampaň na stejný název. Kód by mohl vypadat přibližně takhle:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Campaign.prototype.handleSetCampaignNameCommand = <span class="function"><span class="keyword">function</span>(<span class="params">command</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (command.data.name.length &gt; <span class="number">50</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="built_in">Error</span>(<span class="string">"Campaign name is too long"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (command.data.name !== <span class="keyword">this</span>.name) &#123;</span><br><span class="line">        produceNewEvent(Events.CampaignNameSet, &#123;name: command.data.name&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>V <code>command.data.name</code> máme nový název kampaně. Aby tento kód fungoval, je nutné, aby v době zpracování Commandu byl v <code>this.name</code> aktuální název kampaně. Před samotným zpracováním Commandu proto tzv. <em>instancujeme</em> entitu kampaně, na které se Command provádí. To znamená, že z Monga vytáhneme všechny Události, které se týkají dané kampaně a aplikujeme je na danou entitu. Aplikace Události není o nic složitější než předchozí handle metoda:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Campaign.prototype.applyCampaignNameSet = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = event.data.name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Pokud uživatel desetkrát přejmenoval danou kampaň, vyvolá se desetkrát metoda <em>applyCampaignNameSet</em> a desetkrát se přepíše hodnota <code>this.name</code>. Na konci zpracování ale budeme mít entitu v aktuálním stavu (taháme všechny Události, ne jen <code>CampaignNameSet</code>), což je to, co chceme.</p>
<p>V tuto chvíli můžeme začít zpracovávat command samotný, tzn. že v tuto chvíli zavoláme předchozí metodu <code>handleSetCampaignNameCommand</code>, ve které se už můžeme kvalifikovaně rozhodnout, jestli vypálíme Událost <code>CampaignNameSet</code> nebo jestli to nemá smysl.</p>
<p>V případě nutnosti si můžeme instancovat další entity. Toto instancování entit není zrovna nejrychlejší operace, ale můžeme využít faktu, který jsme uvedli výše – zpracování Commandů nemusí být superrychlé. V současnosti platí, že vytvoření aktuální instance entity = dotaz do Monga na Události pro tu danou entitu, ale až to bude moc pomalé, dá se to relativně jednoduše cachovat.</p>
<p>Má-li vzniknout Událost, musí to být na popud nějakého Commandu. Není možné, aby vznikla Událost bez Commandu.</p>
<h2 id="transakční-zpracování">Transakční zpracování</h2>
<p>Jeden Command může vyprodukovat více než jednu Událost. Příkladem může být třeba pausnutí kampaně. Pausne-li uživatel kampaň, vyvolá se pochopitelně Událost <em>CampaignPausedSet</em>, ale spolu s tím se vyvolá Událost <em>CampaignRunnableStateChanged</em>. <em>CampaignRunnableStateChanged</em> je Událost, která nám říká, jestli může kampaň běžet, nebo jestli je “něco špatně”.</p>
<p>Aby kampaň mohla běžet, musí mít nastaveno kdy a kde má běžet a nesmí být pausnutá. Pokud splňuje všechny tři parametry, pak je i <em>Runnable</em>, pokud jeden z těch parametrů změníme, už není <em>Runnable</em>. Tím, že vypalujeme Událost <em>CampaignRunnableStateChanged</em> si ulehčujeme práci, protože všude jinde už nám pak stačí reagovat na tuto Událost a nemusíme nikde jinde vyhodnocovat logiku, jestli má kampaň všechny potřebné vlastnosti.</p>
<p>Aby vše fungovalo jak má, je nutné, aby se buď vyprodukovaly obě Události, nebo ani jedna. Proto všechny Události vyprodukované jedním Commandem ukládáme jako jednu transakci, tj. jako jeden Mongo objekt.</p>
<h2 id="queries-a-view-buildery">Queries a View Buildery</h2>
<p>Zpracování Commandů sice nemusí být rychlé, ale zpracování Queries ano. Jak to řešíme? Držíme aktuální stav entit v Mongu v jiných kolekcích. Tyto kolekce jsou vytvářeny aplikacemi, které nazýváme <em>View Builder</em>. View Buildery čtou všechny Události a na některé z nich nějak po svém reagují; typicky aktualizují záznam v Mongu a vytvářejí specifické <em>View</em>. Důležité je, že nemáme předepsané, jak takové View má vypadat.</p>
<p>Každý View Builder může vytvářet View, které se hodí pro nějaký konkrétní specifický účel a dokonce je možné, abychom ve dvou různých Views měli prakticky stejná data jenom v jiné struktuře. Důležité je, aby View bylo optimalizované pro čtení. Příklad: máme <em>Campaigns</em> View, ve kterém máme uloženy všechny informace o všech kampaních (jeden dokument = jedna kampaň); včetně názvu. V jiném View máme zase uložená ID všech entit (kampaně, publishery, …) a jejich názvy, nic víc. Toto View používáme v reportech, protože tam máme na vstupu seznam IDéček a potřebujeme je rychle přeložit na lidská jména.</p>
<p>Kód View Builderů je opět velmi jednoduchý. Vlastně jen vytváříte <code>apply</code> metody těch Událostí, na které chcete reagovat:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CampaignViewBuilder.prototype.applyCampaignNameSet = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mongo.updateDocument(event.entityId, &#123;name: event.data.name&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CampaignViewBuilder.prototype.applyCampaignCurrencySet = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mongo.updateDocument(event.entityId, &#123;name: event.data.currency&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>A jak se View Builder dostane k novým Událostem? Řekli jsme si, že po zpracování Commandu se Událost pošle do Event Store. Kromě toho se ještě pošle na <em>Event Bus</em> a skrze něj se Událost dostane do všech View Builderů. Event Bus je implementovaný pomocí <a href="http://zeromq.org/" target="_blank" rel="external">ZeroMQ</a> (zkoušeli jsme i <a href="http://nanomsg.org/" target="_blank" rel="external">nanomsg</a>). ZeroMQ není nic extra složitého, je to jen jednoduchý způsob, jak dostat zprávu z jednoho místa na druhé. Kdybychom celý event sourcing implementovali dnes, asi bychom místo ZeroMQ použili <a href="/kafka/">Kafku</a>.</p>
<p>Naše typická Query je proto implementovaná tak, že se jen podívá do předzpracované Mongo kolekce, položí jednoduchý dotaz a vrátí výsledek.</p>
<h2 id="změna-view">Změna View</h2>
<p>Důležitou výhodou je, že View je jen jiný pohled na Události z Event Store. Což znamená, že když se nám současné View nelíbí, můžeme ho změnit. Stačí jen resetovat View Builder a nechat ho znova přečíst všechny Události a naše View můžeme vypadat úplně jinak. Příklad z praxe: zákazník si může v našem webovém UI vytvořit portfolio, což je vlastně web + podsekce. Může to vypadat třeba takto:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">idnes.cz&#10;    | Kultura&#10;    | Technet&#10;        | Web&#10;        | V&#283;da&#10;    | Ekonomika&#10;    | ...</span><br></pre></td></tr></table></figure>
<p>My jsme se tuto strukturu na poprvé snažili ve View uložit tak, jak ji vidíte. Tj. jeden dokument = celý web včetně všech podsekcí:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    name: "idnes.cz",</span><br><span class="line">    nodes: &#123;</span><br><span class="line">        kultura: &#123; name: "Kultura", nodes: &#123; ... &#125; &#125;,</span><br><span class="line">        technet: &#123; name: "Technet", nodes: &#123; web: &#123; ... &#125; ... &#125; &#125;,</span><br><span class="line">        ekonomika: &#123; name: "Ekonomika", nodes: &#123; ... &#125; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Jenomže časem se ukázalo, že je to blbost a že by bylo lepší uložit to stylem jeden dokument = jedna sekce s tím, že bychom v každém dokumentu měli uložené IDéčka podsekcí. Tj. takto:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; name: "idnes.cz", id: "187c48ce", nodes: ["846ed763", "3c33863f", "106ba878"] &#125;</span><br><span class="line">&#123; name: "Kultura", id: "846ed763", nodes: [...] &#125;</span><br><span class="line">&#123; name: "Technet", id: "3c33863f", nodes: [...] &#125;</span><br><span class="line">&#123; name: "Ekonomika", id: "106ba878", nodes: [...] &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Přepsali jsme <code>PortfolioViewBuilder</code>, aby jinak reagoval na Události týkající se sekcí, přepsali jsme Query, která vracela strukturu portfolio a to bylo vše. Kompletně změnit strukturu dat, ze kterých taháme informace o portfolio, byla práce na dva dny i s testy…</p>
<h2 id="intermezzo">Intermezzo</h2>
<p>V souvislosti s touto změnou jsme přemýšleli, jak efektivně uložit informaci o portfoliu tak, aby bylo možné jednoduchým dotazem vrátit všechny sekce. V předchozí struktuře totiž platí, že idnes.cz ví jen o svých přímých potomcích, tj. o sekcích “Kultura”, “Technet” a “Ekonomika”. Že existují podsekce “Web” a “Věda” zjistíme až z dokumentu “Technet”. Existuje jeden hezký postup, jak jedním dotazem vrátit všechny své potomky, nehledě na úroveň, viz <a href="http://docs.mongodb.org/manual/tutorial/model-tree-structures-with-nested-sets/" target="_blank" rel="external">Model Tree Structures with Nested Sets</a>. Celý princip je pochopitelný z obrázku, který si vypůjčím z odkazované dokumentace:</p>
<div class="figure">
<img src="/images/data-model-example-nested-set.png" alt="">

</div>
<p>Každý uzel stromu si očíslujeme (projdeme strom do hloubky a očíslujeme jak je vidět z obrázku) a když chceme najít všechny potomky uzlu <em>Programming</em>, nalezneme všechny uzly, které mají levé číslo větší než 2 a pravé číslo menší než 11. To je celé. Dobré, ne?</p>
<p>Ale my jsme to nepoužili, protože nám stačí vždy vrátit celý strom, nepotřebujeme nikdy vracet část podstromu.</p>
<h2 id="eventual-consistency">Eventual consistency</h2>
<p>Command je považován za úspěšně zpracovaný, pokud se podařilo všechny vygenerované Události uložit do Event Store. Ve chvíli, kdy Události jsou v Event Store, nejdou už nijak odstranit, nejdou změnit – zůstanou v systému na věky věků. A naopak – pokud se Událost do Event Store nedostane, jako by se nic nestalo.</p>
<p>Jenomže když se Událost dostane do Event Store, tak to ještě neznamená, že se tato změna projevila ve všech částech systému. Někde vedle existuje View Builder, který čte tyto Události a reaguje na ně. Command ale nemá jak zjistit, jestli už na danou Událost reagovaly všechny View Buildery. Proto se může stát, že uživatel přejmenuje kampaň, Command úspěšně projde, uživatel refreshne webové UI a uvidí starý název kampaně. <code>CampaignViewBuilder</code> zkrátka ještě nestihl přečíst <code>CampaignNameSet</code> Událost a uložit do View aktuální stav.</p>
<p>Obecně proto platí, že Query nemusí vrátit aktuální stav systému, který platil v době, kdy byla Query přijata na backendu. Query vrací stav systému, který je uložený ve View a ten může být zpožděný oproti opravdovému stavu. Pokud bychom <em>nějaký čas</em> neprodukovaly žádné Události, tak by se View <em>nakonec</em> do skutečného aktuálního stavu dostalo, až by všechny View Buildery přečetly a zpracovaly všechny Události. Proto se tomuto principu říká <em>Eventual consistency</em>.</p>
<p>Teoreticky bychom mohli zařídit, aby Query vrátila aktuální stav systému. Museli bychom ale Query implementovat tak, aby se nikdy nedotazovala View, ale aby vždy instancovala entity přímo z Event Store. Jenomže tím bychom se zbavili dvou výhod: bylo by to pomalejší a nemohli bychom si data přeskládat a předzpracovat pro konkrétní Query tak, jak zrovna potřebujeme. Celkově bychom tím řádově více zatěžovali Event Store a stal by se z něj ještě větší single point of failure.</p>
<p>Jedním z pricnipů CQRS je oddělení Read (=Queries) a Write (=Commandy) částí systému, takže by nebylo dobré je míchat.</p>
<h2 id="na-jaké-problémy-jsme-narazili">Na jaké problémy jsme narazili</h2>
<p>…si povíme zase příště v dalším článku.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/architektura-event-sourcing/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Jak vypadá naše vývojové prostředí]]></title>
      <link>http://programio.havrlant.cz/docker-vagrant/</link>
      <guid>http://programio.havrlant.cz/docker-vagrant/</guid>
      <pubDate>Mon, 15 Jun 2015 13:02:55 GMT</pubDate>
      <description>
      <![CDATA[<p>Někdy před rokem jsme se rozhodli – snad finálně! – vyřešit problém, že i když používáme multiplatformní technologie jako je Node.JS, tak]]>
      </description>
      <content:encoded><![CDATA[<p>Někdy před rokem jsme se rozhodli – snad finálně! – vyřešit problém, že i když používáme multiplatformní technologie jako je Node.JS, tak ve výsledku má každý vývojář mírně odlišný systém, což způsobuje, že kód, který je funkční na jednom počítači, není funkční na jiném. Cestou se na naše řešení nabalila i jiná pozitiva a o tom všem si v článku povíme.</p>
<p>Před rokem trpělo naše vývojové prostředí přibližně těmito neduhy:</p>
<ul>
<li><p>Stáhli jste si zdrojáky webového UI, rozjeli jste ho, ale nefungovalo vám, protože jste si zapomněli ručně stáhnout jednu chybějící knihovnu.</p></li>
<li><p>Po stažení se sice UI rozběhlo, ale moc toho nefungovalo, protože jste zapomněli spustit backendový server.</p></li>
<li><p>Spustili jste backend, ale nefungovaly statistiky, protože jste u sebe měli starou verzi backendu.</p></li>
<li><p>Napsali jste nějaký kód, vyzkoušeli jste ho, otestovali, commitli&amp;pushli a po chvíli vám přišel email z Jenkinsu, že neprošli unit testy, protože máte lokálně jinou verzi testovacího frameworku než je na Jenskinsu.</p></li>
</ul>
<p>Podobných chyb si jistě dokážete představit stovky.</p>
<h2 id="docker-to-the-rescue">Docker to the rescue!</h2>
<p><a href="http://www.docker.com/" target="_blank" rel="external">Docker</a> asi znáte, je teď kolem něj docela hype. Je to něco trochu podobného jako virtualizovaný OS. Pomocí Dockeru můžeme vytvořit <em>kontejner</em>, což je jakýsi sandbox, který je co možná nejvíce odstíněný od hostitelského OS. Tento kontejner si můžeme různě přizpůsobit a nainstalovat tam své aplikace. Z pohledu uživatele se pak Docker kontejner jeví jako samostatný operační systém se svým file systémem a se svými procesy, ale fakticky v rámci Dockeru žádný virtualizovaný OS neběží a vše zkrátka vykonává hostitelský OS.</p>
<p>Můžeme proto na jednom systému spustit třeba deset kontejnerů, v pěti můžeme provozovat Node.JS aplikace, v dalších třech Java aplikace a ve zbývajících můžeme mít třeba nějaký monitoring. Každý kontejner bude jinak nakonfigurovaný, budou tam běžet jiné procesy a přitom nemusíme virtualizovat deset OS. Hlavní výhoda je v úspoře prostředků. Představte si, že na jednom počítači skutečně virtualizujete deset OS.</p>
<p>Námi v současnosti vyvíjený produkt se skládá z několika desítek Node.JS projektů. Některé z nich jsou HTTP servery, další jsou ZeroMQ servery a zbytek jsou sdílené knihovny. Nevím, jestli se vlezeme do definice <a href="http://martinfowler.com/articles/microservices.html" target="_blank" rel="external">microservices</a>, nicméně platí, že při běžném vývoji nám běží několik desítek Docker kontejnerů.</p>
<p>Tím, že nám všechny aplikace běží v kontejnerech, získáváme jednotné prostředí pro všechny vývojáře. Už žádné rozdílné verze testovacího frameworku – pro testování používáme framework, který je nainstalovaný v kontejneru. Aplikace se spouští v Nodu, který je opět nainstalovaný v kontejneru. Všechny tyto závislosti se jednou nastaví v konfiguraci daného kontejneru a programátoři už do toho nehrabou.</p>
<p>Veškeré externí závislosti, které se nějak využívají pro běh našich aplikací, máme vždy ve stejných verzích, takže pokud se naše aplikace chová na dvou různých počítačích jinak, obvykle to znamená, že si někdo nesynchronizoval změny v kódu.</p>
<p>No jo, jenže Docker zatím běží pouze na Linuxu a my máme v týmu i Windowsáře a Macaře. Co s tím?</p>
<h2 id="vagrant-to-the-rescue">Vagrant to the rescue!</h2>
<p><a href="https://www.vagrantup.com/" target="_blank" rel="external">Vagrant</a> je jakýsi wrapper nad virtualizačními programy jako je VirtualBox nebo VMware apod. Umožňuje nám pomocí jednoduchého konfiguračního souboru nakonfigurovat běžící virtualizovaný OS. Vagrant jsme proto využili tak, že v něm virtualizujeme CentOS, ve kterém spouštíme naše Docker kontejnery. Ráno zkrátka přijdeme do práce, spustíme Vagrant, v něm se spustí kontejnery a jedeme. Všem vývojářům, nehledě na OS, běží všechny aplikace v identickém prostředí.</p>
<p>Další příjemný bonus je jednoduchost prvotní instalace. Pokud přijde do týmu nový člověk, jako první si nainstaluje samotný Vagrant, pak si stáhne konfigurační soubory z repozitáře definující naše vývojové prostředí a jedním příkazem <code>vagrant install</code> nainstaluje všechny aplikace potřebné pro vývoj současného produktu. Druhým příkazem <code>vagrant up</code> pak prostředí spustí. To je celé.</p>
<p>Pokud se během toho procesu nic nepokazí, má někdy po dvou hodinách funkční a běžící aplikaci. Může si v prohlížeči zobrazit naše UI psané v Angularu, tam si může zkusit založit novou kampaň, což odchytí backend – HTTP server, který přes ZeroMQ dá vědět ostatním částem systému o nové kampani. Ty přijmou zprávu a aktualizují nějaké údaje v Mongu, které nám také běží v jednom z kontejnerů. A tak dále.</p>
<p>Protože máme v Dockeru nainstalované i Mongo, což je naše hlavní databáze, <strong>má každý vývojář na svém počítači úplnou a nezávislou instalaci celého systému</strong>. Pokud například v UI naklikám novou kampaň, data o ní bude mít v Mongu jen ten daný vývojář a nebude tím otravovat ostatní.</p>
<p>Stejně snadno lze psát i integrační testy. Po restartu celého prostředí se všechny kontejnery dostanou do původního stavu, takže můžeme snadno napsat nějaké scénáře a otestovat, zda vše proběhlo jak mělo. Tyto integrační testy si pak může pustit kterýkoliv vývojář, když si například chce ověřit, že novou fíčurou nezničil nějakou starou.</p>
<h2 id="a-co-statistiky">A co statistiky?</h2>
<p>Z <a href="/tags/bigdata/">předchozích článků</a> víte, že pro statistiky používáme hromadu různých služeb, které jsou navíc napsané v Javě, takže jsou to docela žrouty paměti. Nakonec se ukázalo, že to zase tak velký problém nebyl a zprovoznili jsme všechny části našich statistik přes Docker kontejnery, takže je opravdu možné provozovat celý náš systém na svém počítači bez toho, aby jakákoliv součást byla sdílená. Stručně k jednotlivým službám:</p>
<ul>
<li><p><a href="/kafka/">Kafce</a> jsme pouze nastavili, aby nežrala tolik paměti přes <code>-Xmx</code> a <code>-Xms</code> parametry a všechny topicy vytváříme pro jednoduchost pouze s jednou partition a bez replikací.</p></li>
<li><p><a href="/samza/">Samzy</a> jsme se báli nejvíce, protože jsme standardně pouštěli přes Hadoop a že by se nám do Vagrantu chtělo instalovat ještě Hadoop, to se říci nedalo. Nicméně Samza lze pro testovací účely spustit jako obyčejný Java proces, což jsme využili a spouštíme ji právě tak.</p></li>
<li><p><a href="/druid-io">Druid</a> potřebuje k běhu hodně procesů a nějaké ty externí závislosti. Nakonec jsme ale žádnou závislost navíc neinstalovali. Druid je robustní, takže pokud nějaká služba neběží, Druid se snaží stále fungovat dále v omezených podmínkách. To nám pro testovací účely stačí. Segmetny lze buď neukládat, nebo je lze ukládat na lokální disk.</p></li>
</ul>
<p>V kontejnerech nám tedy běží light-weight verze statistik, která se ale chová identicky jako na ostrých serverech, což je přesně to, co potřebujeme.</p>
<h2 id="další-vychytávky">Další vychytávky</h2>
<ul>
<li><p>Všechny Node.JS aplikace se spouští pod <a href="https://github.com/isaacs/node-supervisor" target="_blank" rel="external">supervisorem</a>, který sleduje změny ve zdrojácích a při každé změně restartuje danou službu.</p></li>
<li><p>Jak jsem psal výše, celý náš systém se skládá z několika desítek běžících aplikací či knihoven. Jedním z problémů je, jak zařídit, aby se změna v jedné knihovně automaticky projevila ve všech aplikacích, které tuto knihovnu používají. V kontejnerech máme projekty nainstalované a prolinkované tak, aby se přesně toto dělo.</p></li>
<li><p>Protože díky Vagrantu máme všichni všechny zdrojáky na stejném místě, mohli jsme snadno napsat skripty, které třeba kontrolují, jestli jsme něco nezapomněli commitnout, které umí stáhnout všechny změny z repozitářů nebo které umí spustit unit testy pro danou aplikaci, nehledě na to, jestli je to webová aplikace, nebo serverová. Protože se tyto shell skripty vykonávají uvnitř Vagrantu, automaticky fungují všem, nehledě na OS.</p></li>
<li><p>Díky Dockeru si můžeme snadno zobrazit logy z každé aplikace, tail-efnout se aktuální logy nebo restartovat kontejner, pokud je zrovna nějaký nemocný.</p></li>
</ul>
<h2 id="na-jaké-problémy-jsme-narazili">Na jaké problémy jsme narazili</h2>
<ul>
<li><p>Dlouhodobě jsme měli největší problémy s DNS. Všechny naše projekty běží na nějaké doméně, takže třeba Mongo běží na <code>mongo.adserver.dev</code>, Ad Selector běží na <code>adselector.adserver.dev</code> a podobně. Jenomže aby to fungovalo, potřebujeme něco, co bude dané domény překládat. K tomu jsme používali třeba <a href="https://registry.hub.docker.com/u/skynetservices/skydns/" target="_blank" rel="external">SkyDNS</a>, což sice fungovalo, ale občas se stávalo, že SkyDNS nastartoval později než bylo potřeba a na několik prvních dotazů zkrátka nestihl odpovědět. Částečně to bylo způsobeno i vysokým zatížením systému, viz další bod.</p></li>
<li><p>Občas jsou problémy s výkonem, hlavně kvůli všem těm watcherům, které sledují zdrojáky. To jsme ze začátku dlouho ladili, než se to vyladilo k plné spokojenosti všech. Na Linuxech jsme nakonec skončili u <a href="http://docs.vagrantup.com/v2/synced-folders/nfs.html" target="_blank" rel="external">NFS</a>, se kterým je vše nejrychlejší a nevytěžuje to systém. Při běžném provozu mi celý Vagrant vytěžuje asi 40 % jednoho CPU a přibližně 4 GB RAM.</p></li>
<li><p>Nebylo úplně jednoduché zařídit, aby se ty desítky kontejnerů spustily ve správném pořadí, ve správném čase, až když běží jiná služba apod.</p></li>
<li><p>Přesměrování portů. Pokud nějaká aplikace běží v Dockeru a běží na nějakém portu, musíme zařídit, aby se ostatní mohli na tento port dostat. Chceme-li, ať se vidí jednotlivé kontejnery mezi sebou, stačí to uvést v Dockerfilu. My ale občas chceme poslat ze svého lokálního počítače dotaz na nějaký server běžící v kontejneru. Proto musí ještě nastavit, ať je port viditelný i z venku, ne jen z jiného kontejneru. No jo, jenže „z venku“ znamená ve Vagrantu, protože Docker kontejner běží ve Vagrantu. Proto musíme port přesměrovat ještě jednou, mezi naším lokálním strojem a Vagrantem. Jo, je to přesně takový chaos, jaký z tohoto odstavce máte pocit.</p></li>
<li><p>Co se týče operačních systémů, nejhorší zkušenosti máme s OS X, nejlepší s Linuxem. Windows je tak nějak mezi.</p></li>
</ul>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/docker-vagrant/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Jak Druid.io agreguje data]]></title>
      <link>http://programio.havrlant.cz/druid-io-ingest/</link>
      <guid>http://programio.havrlant.cz/druid-io-ingest/</guid>
      <pubDate>Tue, 02 Jun 2015 18:38:39 GMT</pubDate>
      <description>
      <![CDATA[<p>Z <a href="/druid-io-architektura">minulého dílu</a> víme, že <a href="http://druid.io/" target="_blank" rel="external">Druid</a> typicky]]>
      </description>
      <content:encoded><![CDATA[<p>Z <a href="/druid-io-architektura">minulého dílu</a> víme, že <a href="http://druid.io/" target="_blank" rel="external">Druid</a> typicky čte zprávy z nějakého messaging systému, tvoří segmenty a tyto segmenty poté proplouvají různými částmi systému. Dnes se podíváme na to, jak vypadají data v jednotlivých segmentech.</p>
<h2 id="agregujeme">Agregujeme</h2>
<p>Druid je sloupcová databáze, pro ilustraci si můžeme zpracovaná data zobrazit jako tabulku. Pokud bychom Druidu poslali tyto dvě zprávy</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">timestamp</span>": <span class="value"><span class="string">"2015-03-17T20:40:00Z"</span></span>,</span><br><span class="line">    "<span class="attribute">campaignId</span>": <span class="value"><span class="string">"336"</span></span>,</span><br><span class="line">    "<span class="attribute">web</span>": <span class="value"><span class="string">"csfd.cz"</span></span>,</span><br><span class="line">    "<span class="attribute">browser</span>": <span class="value"><span class="string">"Chrome"</span></span><br><span class="line"></span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">timestamp</span>": <span class="value"><span class="string">"2015-03-17T20:40:02Z"</span></span>,</span><br><span class="line">    "<span class="attribute">campaignId</span>": <span class="value"><span class="string">"1344"</span></span>,</span><br><span class="line">    "<span class="attribute">web</span>": <span class="value"><span class="string">"idnes.cz"</span></span>,</span><br><span class="line">    "<span class="attribute">browser</span>": <span class="value"><span class="string">"Chrome"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>v Druidu by se poskládala tato tabulka:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:40:00Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:40:02Z</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
</tr>
</tbody>
</table>
<p>Dosud nic světoborného. Zázraky by se začaly dít, až by Druid přečetl například tuto zprávu:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">timestamp</span>": <span class="value"><span class="string">"2015-03-17T20:40:27Z"</span></span>,</span><br><span class="line">    "<span class="attribute">campaignId</span>": <span class="value"><span class="string">"336"</span></span>,</span><br><span class="line">    "<span class="attribute">web</span>": <span class="value"><span class="string">"csfd.cz"</span></span>,</span><br><span class="line">    "<span class="attribute">browser</span>": <span class="value"><span class="string">"Chrome"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>…protože ta se od úplně první zprávy liší pouze v čase (20:40:<strong>00</strong> vs 20:40:<strong>27</strong>), ostatní vlastnosti jsou stejné. Jsou to dvě stejné události, které pouze nastaly v jiný čas. Druid by samozřejmě mohl tuto zprávu uložit jako třetí řádek do tabulky</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:40:<strong>00</strong>Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:40:02Z</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-03-17T20:40:<strong>27</strong>Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
</tbody>
</table>
<p>…ale efektivnější bude, když provede <em>agregaci</em> a jen si u prvního řádku uloží, že tato událost nastala dvakrát:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
<td align="left">1</td>
</tr>
</tbody>
</table>
<p>Tyto tabulky reprezentují stejná data, u druhé jsme pouze ztratili informaci o tom, kdy která událost přesně nastala. Všimněte si, že ve druhé tabulce už máme v timestampu pouze hodinu, bez přesného času. První řádek tabulky říká, že v čase 20:00:00 až 20:59:59 nastaly pro kampaň 336 na ČSFD dvě imprese.</p>
<p>Druid při agregaci postupuje tak, že porovnává hodnoty ve všech sloupcích a pokud se zprávy liší pouze v čase, spojí tyto zprávy do jedné a za každou zprávu inkrementuje sloupec <em>impressions</em>. Název nově vzniklého sloupce specifikujeme ve specFilu:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">"aggregators": [</span><br><span class="line">    &#123;</span><br><span class="line">        "type": "count",</span><br><span class="line">        "name": "impressions" </span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>Tímto vytvoříme agregátor <code>count</code>, který se bude ukládat do sloupečku <em>impressions</em>. Druid zná tři druhy sloupců:</p>
<ul>
<li><strong>čas</strong> – reprezentuje, pro kterou hodinu jsou data platná. V příkladu jde o sloupec “timestamp”.</li>
<li><strong>dimenze</strong> – to jsou naše „syrová“ data, jsou to sloupce <code>campaignID</code>, <code>web</code> a <code>browser</code>.</li>
<li><strong>metrika</strong> – vypočtené hodnoty, sloupec <code>impressions</code>.</li>
</ul>
<h2 id="další-agregátory">Další agregátory</h2>
<p>Druid má podporu pro několik dalších <a href="http://druid.io/docs/0.7.1.1/Aggregations.html" target="_blank" rel="external">agregátorů</a>. Mohli bychom například přidat informaci o ceně za impresi; tabulka před agregací by vypadala takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:40:00Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">0.17</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:40:02Z</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
<td align="left">0.19</td>
</tr>
<tr class="odd">
<td align="left">2015-03-17T20:40:27Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">0.43</td>
</tr>
</tbody>
</table>
<p>Podle toho, co dosud víme, by se žádné řádky nespojily v jeden, protože všechny řádky se liší přinejmenším v ceně. To je ale docela nešikovné. Proto můžeme přidat <code>doubleSum</code> agregátor, který zařídí, aby se během agregace hodnoty ve sloupcích sečetly:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">type</span>": <span class="value"><span class="string">"doubleSum"</span></span>,</span><br><span class="line">    "<span class="attribute">name</span>": <span class="value"><span class="string">"price"</span></span>,</span><br><span class="line">    "<span class="attribute">fieldName</span>": <span class="value"><span class="string">"price"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>Vlastnost <code>fieldName</code> říká, z kterého sloupečku má hodnoty číst a vlastnost <code>name</code> do jakého sloupečku má Druid agregovaná data ukládat. Protože máme oba názvy stejné, budou se data ve sloupečku price přepisovat. Nyní už může dojít k agregaci. Druid ví, že má porovnávat pouze hodnoty ve sloupcích <code>campaignId</code>, <code>web</code> a <code>browser</code>, zatímco sloupec <code>price</code> má vždy jen sečíst. Proto spojí první a třetí řádek do jednoho:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
<th align="left">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
<td align="left">0.6</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
<td align="left">1</td>
<td align="left">0.19</td>
</tr>
</tbody>
</table>
<p>Nyní už ve sloupci <code>price</code> nemáme cenu jedné imprese, ale součet cen všech impresí v daném časovém úseku, pod danou kampaní, na daném webu a s daným prohlížečem. Z takové agregované tabulky už bychom samozřejmě nikdy nezjistili, jaká byla cena za jednu impresi.</p>
<h2 id="unikátní-uživatelé">Unikátní uživatelé</h2>
<p>Co když do zprávy přidáme i informaci o tom, kterému uživateli se banner zobrazil? Data před agregací:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">userId</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:40:00Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">40BBF8</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:40:27Z</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">9896AA</td>
</tr>
</tbody>
</table>
<p>Události nastaly ve stejnou hodinu, ale liší se v hodnotě <code>userId</code>. Druid by proto tyto dvě zprávy uložil do dvou řádků. To je ale dost prekérní situace, protože nyní agregace proběhne jen tehdy, týkají-li se dva řádky stejného uživatele, což nemusí nastat moc často.</p>
<p>V tuto chvíli se musíme zamyslet nad tím, na co data ve skutečnosti potřebujeme. Jde-li nám jen o statistické dotazy typu „kolik unikátních uživatelů vidělo reklamu XYZ na webu ZXY“, můžeme si pomoci tím, že použijeme pravděpodobností algoritmy. Ty nám z dostupných dat nespočítají přesný počet unikátních uživatelů, ale odhadnou počet unikátních uživatelů s nějakou chybou (typicky pod dvě procenta). To je většinou přijatelné. Výhodou pravděpodobnostních algoritmů pak je, že mají nižší paměťové nároky a jsou rychlejší.</p>
<p>Druid zná <a href="/hyperloglog">HyperLogLog</a>, což je v současné době nejlepší algoritmus pro odhadování unikátních hodnot v multimnožině. Přidáme do našeho specFilu nový agregátor <code>hyperUnique</code>:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">type</span>" : <span class="value"><span class="string">"hyperUnique"</span></span>, </span><br><span class="line">    "<span class="attribute">name</span>" : <span class="value"><span class="string">"uniqueUsers"</span></span>, </span><br><span class="line">    "<span class="attribute">fieldName</span>" : <span class="value"><span class="string">"userId"</span> </span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>Abyste plně pochopili, co se teď bude dít, je nutné vědět jak funguje <a href="/hyperloglog">HyperLogLog</a> a hlavně jak funguje <a href="/sjednoceni-hyperloglogu">sjednocení hyperloglogu</a>. Stručně řečeno, HyperLogLog je algoritmus, který převádí hodnoty ve sloupci na vektory, například na [0, 0, 3, 0], z nichž lze později odhadnout počet unikátních hodnot v daném sloupci.</p>
<p>Velkou výhodou je, že máme-li dva vektory [0, 3, 0, 0] a [0, 2, 0, 0], které reprezentují dvě různé hodnoty <code>A</code> a <code>B</code>, můžeme reprezentovat množinu <code>{A, B}</code> tak, že vektory „sjednotíme“ (vypočítáme maximum po složkách):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 3, 0, 0]&#10;[0, 2, 0, 0]&#10;------------&#10;[0, 3, 0, 0]</span><br></pre></td></tr></table></figure>
<p>Druid při agregaci vypočte HLL vektor a ten uloží do tabulky do sloupce <code>uniqueUsers</code> a <strong>hodnotu <code>userId</code> zahodí</strong>. Před finální agregací bude tabulka vypadat takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
<th align="left">uniqueUsers</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">1</td>
<td align="left">[0, 3, 0, 0]</td>
</tr>
<tr class="even">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">1</td>
<td align="left">[0, 2, 0, 0]</td>
</tr>
</tbody>
</table>
<p>Protože HLL vektory umíme <a href="/sjednoceni-hyperloglogu">sjednotit</a>, můžeme tyto dva řádky spojit do jednoho. Výsledná agregovaná tabulka by vypadala takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
<th align="left">uniqueUsers</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
<td align="left">[0, 3, 0, 0]</td>
</tr>
</tbody>
</table>
<p>Z těchto dat umíme vyčíst přesný počet impresí pro danou kampaň, daný web a daný prohlížeč za daný čas. Z daných dat také umíme získat přibližný počet unikátních uživatelů, kterým se tento banner zobrazil. Důležité je, že i když poroste počet zobrazení daného banneru, stále nám pro uložení stačí jeden řádek. Pokud by kampaň vyprodukovala 15 874 impresí u 6 098 unikátních uživatelů, mohl by řádek vypadat nějak takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
<th align="left">uniqueUsers</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-17T20:00:00</td>
<td align="left">336</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">15 874</td>
<td align="left">[15, 17, 10, 14]</td>
</tr>
</tbody>
</table>
<p>Počet impresí máme v tabulce přesně, počet unikátů by nám Druid odhadl z vektoru <code>[15, 17, 10, 14]</code>. Přitom nikde nemáme uložená IDéčka uživatelů. Nepotřebujeme je.</p>
<p>Sloupec <code>uniqueUsers</code> představuje další metriku v naší tabulce.</p>
<h2 id="časová-granularita">Časová granularita</h2>
<p>Dosud jsme stále pracovali s tím, že se data agregují v rámci jedné hodiny. Jako většina věcí, i toto je v Druidu nastavitelné:</p>
<pre><code>&quot;granularitySpec&quot; : {
    &quot;type&quot; : &quot;uniform&quot;,
    &quot;segmentGranularity&quot; : &quot;day&quot;,
    &quot;queryGranularity&quot; : &quot;hour&quot;
}</code></pre>
<p>Teď se budeme zabývat vlastností <code>queryGranularity</code>, která nám udává, za jaký časový úsek se budou agregovat zprávy. Nastavíme-li hodinovou queryGranularity, pak se nám zprávy budou agregovat vždy v rámci jedné hodiny (14:00 až 14:59:59…), tzn., že za jeden den budeme mít pro každou kombinaci dat 24 různých řádků pro 24 hodin. Jeden příklad lepší než tisíc slov (A pro jednoduchost vynecháme sloupec userId, protože Hyperloglogu stejně nikdo nerozumíte):</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-20T18:50:00Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T18:51:09Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-03-20T18:51:12Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T18:51:35Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-03-20T18:51:35Z</td>
<td align="left">13650</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T18:51:35Z</td>
<td align="left">13650</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T19:14:27Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-03-20T19:15:38Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T19:16:01Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
</tbody>
</table>
<p>tuto tabulku by Druid agregoval na tuto výslednou tabulku:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">Impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-20T18:00:00Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T18:00:00Z</td>
<td align="left">13650</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T19:00:00Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
</tr>
</tbody>
</table>
<p>Pokud bychom nastavili <code>queryGranularity</code> na <code>day</code>, vypadala by výsledná agregovaná tabulka takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">timestamp</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">Impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-03-20T00:00:00Z</td>
<td align="left">36708</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">2015-03-20T00:00:00Z</td>
<td align="left">13650</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
</tbody>
</table>
<p>Hodnota queryGranularity nám určuje největší možnou granularitu. Nastavíme-li queryGranularity na hodinu, nebudeme už z Druidu schopni vytáhnout data po minutách. Ale můžeme vytáhnout data po dnech, protože to je menší granularita.</p>
<p>Vlastnost segmentGranularity udává velikost jednoho <a href="/druid-io-architektura">segmentu</a>, což jsme řešili v předchozím článku. V jednom denním segmentu protože můžeme mít například 24 hodinových bloků dat.</p>
<h2 id="co-druid-ukládá">Co Druid ukládá</h2>
<p>Důležité je, že <strong>Druid ukládá pouze agregovaná data</strong>. Původní zprávy, ze kterých byla data vybudovaná, neukládá a není možné se k nim přes Druida nějak dostat. Pokud jednou uložíme data s hodinovou granularitou, nedokážeme z Druidu žádným způsobem vytáhnout počet impresí za čas 13:15 až 13:30, budeme moci vytáhnout pouze data za 13:00 až 14:00. Nemůžeme ani říci Druidu, aby přepočítal data z minulého týdne, protože jsme tam našli chybu – Druid surová data nemá a proto nemůže vše znova přepočítat.</p>
<p>Proto se často původní surová data nezahazují, ale ukládají se někam jinam, například do <a href="http://en.wikipedia.org/wiki/Apache_Hadoop#HDFS" target="_blank" rel="external">HDFS</a> nebo do <a href="http://aws.amazon.com/s3/" target="_blank" rel="external">Amazon S3</a>. Tím máme zaručeno, že když se něco v Druidu pokazí, že můžeme vzít data z HDFS a poslat je znovu do Druidu. Třeba když zjistíme, že by se nám vlastně hodila minutová granularita. Na přesun dat z Kafky do HDFS používáme <a href="https://github.com/linkedin/camus" target="_blank" rel="external">Camus</a>.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/druid-io-ingest/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Architektura Druid.io]]></title>
      <link>http://programio.havrlant.cz/druid-io-architektura/</link>
      <guid>http://programio.havrlant.cz/druid-io-architektura/</guid>
      <pubDate>Mon, 11 May 2015 16:42:56 GMT</pubDate>
      <description>
      <![CDATA[<p><a href="http://druid.io/" target="_blank" rel="external">Druid.io</a> je distribuovaná databáze napsaná v Javě, která se skládá z několi]]>
      </description>
      <content:encoded><![CDATA[<p><a href="http://druid.io/" target="_blank" rel="external">Druid.io</a> je distribuovaná databáze napsaná v Javě, která se skládá z několika částí. Těmto částem říkáme <em>uzly</em> (<em>nodes</em>) a jsou to separátní Java procesy. Základní instalace Druidu potřebuje alespoň čtyři uzly: Realtime, Historický, Koordinátor a Broker, přičemž každý z nich má jinou zodpovědnost. V článku si všechny čtyři uzly představíme.</p>
<h2 id="dostáváme-data-do-druidu">Dostáváme data do Druidu</h2>
<p>Není typické, aby aplikace, které generují data (například naše Node.JS HTTP servery) posílaly data přímo do Druidu. Místo toho využívají nějakého messaging systému, do kterého zprávy posílají, a odtud se poté dostanou až do Druidu. My používáme <a href="/kafka/">Kafku</a>.</p>
<p>Začnu s popisem starší verze Druidu, kterou už sice nepoužíváme, ale jednodušeji se vysvětluje. Vstupním bodem celého Druid clusteru je <a href="http://druid.io/docs/0.7.0/Realtime.html" target="_blank" rel="external">Realtime uzel</a>.</p>
<h2 id="realtime-uzel">Realtime uzel</h2>
<p>Realtime uzel je služba, která se stará o načítání zpráv z Kafky nebo z jiného messaging systému. Spouští se jako obyčejný Java proces:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java io.druid.cli.Main server realtime</span><br></pre></td></tr></table></figure>
<p>V praxi by tam ale byla ještě hromada dalších parametrů, kterými se teď nebudeme zatěžovat. Realtime uzel potřebuje na vstupu ještě <a href="http://druid.io/docs/0.7.0/Realtime-ingestion.html" target="_blank" rel="external">konfigurační soubor</a>, tzv. “specFile”. My si z něj ukážeme jen pár částí. Začneme tou, kde se definuje, odkud se budou číst data:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">type</span>": <span class="value"><span class="string">"kafka-0.8"</span></span>,</span><br><span class="line">    "<span class="attribute">feed</span>": <span class="value"><span class="string">"impressions"</span></span>,</span><br><span class="line">    "<span class="attribute">parser</span>": <span class="value">&#123;</span><br><span class="line">    	"<span class="attribute">data</span>": <span class="value">&#123;</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"json"</span></span><br><span class="line">        </span>&#125;</span>,</span><br><span class="line">        "<span class="attribute">timestampSpec</span>": <span class="value">&#123;</span><br><span class="line">            "<span class="attribute">column</span>": <span class="value"><span class="string">"timestamp"</span></span>,</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"iso"</span></span><br><span class="line">        </span>&#125;        </span><br><span class="line">    </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>SpecFile říká, ať Realtime uzel čte zprávy z Kafka topicu <em>impressions</em> a ať očekává na vstupu JSON, který má vlastnost <em>timestamp</em>, ve kterém je uložen čas, kdy událost nastala. Toto je opravdu důležité – do Druidu lze dostat jen <em>události</em>, které nastaly v určitý čas a Druid potřebuje vědět kdy. Zprávy bez timestampu Druid zahodí.</p>
<!-- Tím řekneme Realtime uzlu, aby použil ovladače pro Kafka 0.8 a začal číst Kafka topic *impressions*. Vlastnost "data" říká, vstupní data očekáváme v JSON formátu. Vlastnost "timestampSpec" pak říká, že tyto JSONy budou mít v property "timestamp" časové razítko a to bude ve formátu ISO, tj. něco jako "2015-03-17T20:40:00Z". Schéma dat nikde nespecifikujeme, tj. nikde neříkáme, jaké property má JSON mít. 

Jen pozor na to, že když do topicu pošlete zprávu, která není JSON nebo nemá platné časové razítko v dané vlastnosti, tak Druid tuto zprávu **zahodí**. Když jsme Druida zkoušeli úplně poprvé, tak jsme tam prostě valili nějaké JSONy, které jsme měli po ruce, ale do Druidu se žádná data nedostala, protože neměla platné časové razítko. Validní vstup, který v našem případě reprezentuje zobrazení jednoho reklamního banneru, tj. impresi, by mohl vypadat třeba takto:


<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">timestamp</span>": <span class="value"><span class="string">"2015-03-17T20:40:00Z"</span></span>,</span><br><span class="line">    "<span class="attribute">campaignId</span>": <span class="value"><span class="string">"336"</span></span>,</span><br><span class="line">    "<span class="attribute">web</span>": <span class="value"><span class="string">"csfd.cz"</span></span>,</span><br><span class="line">    "<span class="attribute">browser</span>": <span class="value"><span class="string">"Chrome"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>–&gt;</p>
<p>Po spuštění začne uzel číst zprávy z topicu, zpracovávat je a ukládat je. V tuto chvíli jsou data dotazovatelná – mohli bychom poslat přes REST API dotaz na Realtime uzel a získat třeba počet zpracovaných impresí.</p>
<h2 id="segment">Segment</h2>
<p>Během čtení zpráv začne Realtime uzel vytvářet něco, čemu se říká <strong>segment</strong>. Segment je soubor, který obsahuje Druidem zpracovaná, indexovaná a agregovaná data za určitý čas. Zpracovaný segment už je dále neměnný, není ho možné updatovat.</p>
<p>Kolik dat bude v jednom segmentu obsaženo závisí na hodnotě segmentGranularity:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#34;granularitySpec&#34; : &#123;&#10;    &#34;type&#34; : &#34;uniform&#34;,&#10;    &#34;segmentGranularity&#34; : &#34;day&#34;,&#10;    &#34;queryGranularity&#34; : &#34;hour&#34;&#10;&#125;&#10;``` &#10;&#10;Druid by pro ka&#382;d&#253; nov&#253; den za&#269;al vytv&#225;&#345;et nov&#253; segment. Pro jednoduchost m&#367;&#382;eme nyn&#237; p&#345;edpokl&#225;dat, &#382;e m&#225;me-li denn&#237; segmenty, pak se pro ka&#382;d&#253; den vytvo&#345;&#237; jeden soubor, kter&#253; obsahuje v&#353;echna data z tohoto dne. Tento soubor se pak nakop&#237;ruje na n&#283;kter&#253; z Druid&#237;ch server&#367; a tento server zodpov&#237;d&#225; dotazy pro dan&#253; den.&#10;&#10;&#10;## &#381;ivotn&#237; cyklus segmentu&#10;&#10;Dejme tomu, &#382;e nastartujeme Realtime uzel v pond&#283;l&#237; v 11:15. Realtime uzel by za&#269;al okam&#382;it&#283; vytv&#225;&#345;et denn&#237; segment pro pond&#283;l&#237; a tento segment by zpracov&#225;val a&#382; do p&#367;lnoci. Po celou dobu by data za pond&#283;l&#237; byla na Realtime uzlu, ale byla by dotazovateln&#225; p&#345;es REST API. &#10;&#10;Dal&#353;&#237; zaj&#237;mav&#225; v&#283;c se za&#269;ne d&#237;t na konci dne, o p&#367;lnoci. Realtime uzel se za&#269;ne p&#345;ipravovat na dal&#353;&#237; segment. V 00:00 se vytvo&#345;&#237; dal&#353;&#237; segment a zpr&#225;vy z &#250;terka se zpracov&#225;vaj&#237; do tohoto nov&#233;ho &#250;tern&#237;ho segmentu.&#10;&#10;&#60;!---&#10;No jo, ale co kdy&#382; n&#225;m p&#345;ijde n&#283;jak&#225; zpr&#225;va pozd&#283;? Co kdy&#382; se n&#283;jak&#225; zpr&#225;va s &#269;asem 23:59:57 dostane k Druidu a&#382; v &#269;ase 00:01:12 dal&#353;&#237;ho dne? Druid umo&#382;&#328;uje nastavit &#269;as, po kter&#253; bude je&#353;t&#283; &#269;ekat na takto zpo&#382;d&#283;n&#233; zpr&#225;vy:</span><br></pre></td></tr></table></figure>
<p>“windowPeriod”: “PT10m” ```</p>
<p>Tímto bychom nastavili, že ještě deset minut po půlnoci bude čekat na zpožděné zprávy z předchozího dne. Zprávy s timestampem z pondělka by šly do pondělího segmentu, zprávy s timestampem z úterka do úterního. (Nezpomeňte, že každá zpráva si sama s sebou nese informaci o tom, kdy vznikla.)</p>
<p>Pondělní segment je pak považován za uzavřený a Druid už do něj žádnou další zprávu nepustí. Místo toho vytvoří <em>nový pondělní segment</em>. Dosud totiž Druid potřeboval mít pondělní data v takové struktuře, do které mohl snadno přidávat nově příchozí zprávy. Jenomže teď už je úterý a do pondělního segmentu žádná další data nepřijdou – proto Druid přechroustá všechny pondělní zprávy a vytvoří nové indexy. Celkově vytvoří novou strukturu, která bude optimalizována pro vyhledávání. –&gt;</p>
<p>Pondělní segment je v tuto chvíli považován za uzavřený a Druid už do něj žádnou další zprávu nevloží. Díky tomu může Druid pozměnit strukturu pondělního segmentu tak, aby data v něm byla optimalizována pro vyhledávání. Po uzavření pondělního segmentu proto Druid přeskládá všechny zprávy a vytvoří nové indexy, aby se v segmentu dalo rychle vyhledávat.</p>
<p>Tento upravený segment odešle do <em>deep storage</em>. Tím Druiďáci nazývají vzdálené úložiště, ve kterém jsou archivovány všechny segmenty. V současné době je podporováno <a href="http://hortonworks.com/hadoop/hdfs/">HDFS</a> a <a href="http://aws.amazon.com/s3/">Amazon S3</a>. Tohle je podstatná informace – Druid neobsahuje nic jako trvalé úložiště pro zpracované segmenty. Samotná data musíme uložit jinam. Na druhou stranu napojení na ostatní služby není nijak těžké. My používáme HDFS.</p>
<p>Metadata o novém segmentu uloží Druid do MySQL. Základní schéma vypadá takto:</p>
<div class="figure">
<img src="/images/druid/deepstorage.svg" alt="" />

</div>
<!--
## MySQL
 
Nyní se podíváme na to, jak spolu jednotlivé části Druidu komunikují. Protože Realtime uzel odeslal segment do deep storage, musí o tom dát vědět zbytku clusteru. To se děje pomocí dalších dvou externích závislostí: MySQL a ZooKeeper. Po vytvoření segmentu by Realtime uzel uložil metadata o segmentu do MySQL. Část dat z naší MySQL: 


| dataSource  | start                    | end                      | created_date             |
|-------------|--------------------------|--------------------------|--------------------------|
| ssp-auction | 2015-03-13T11:00:00.000Z | 2015-03-13T12:00:00.000Z | 2015-03-13T12:53:25.540Z |
| ssp-auction | 2015-03-13T12:00:00.000Z | 2015-03-13T13:00:00.000Z | 2015-03-13T14:04:42.668Z |
| ssp-auction | 2015-03-13T13:00:00.000Z | 2015-03-13T14:00:00.000Z | 2015-03-13T15:16:01.205Z |
| ssp-auction | 2015-03-13T14:00:00.000Z | 2015-03-13T15:00:00.000Z | 2015-03-13T16:10:22.088Z |
| ssp-auction | 2015-03-13T15:00:00.000Z | 2015-03-13T16:00:00.000Z | 2015-03-13T16:51:05.289Z |


Vidíme, že v databázi jsou uložené informace jako je čas vzniku segmentu a který interval segment obsluhuje. Kromě těch sloupečků je v databázi ještě některé další sloupečky jako `version` nebo `used`. Z databáze si můžeme všimnout, že vytvoření segmentu není úplně nenáročná činnost: V prvním řádku máme segment, který má data pro interval 11:00--12:00, přitom tento segment vznikl až v 12:53. Window period jsme měli na 15 minutách, takže Druidu trvalo nějakých 38 minut, než segment vytvořil, tj. než zkonvertoval původní strukturu do immutable struktury. 

V tuto chvíli je segment na dvou místech: na Realtime uzel a v deep storage. Protože Realtime uzel už začíná zpracovávat další segment, chce se toho předchozího zbavit. Pokud bychom v tuto chvíli totiž položili dotaz, který by dotazoval data z pondělí, stále by na to odpovídal Realtime uzel. 
-->
<h2 id="koordinátor-a-historický-uzel">Koordinátor a Historický uzel</h2>
<p>Nyní přichází na řadu další dva druidí uzly: <a href="http://druid.io/docs/0.7.0/Coordinator.html" target="_blank" rel="external">Koordinátor</a> a <a href="http://druid.io/docs/0.7.0/Historical.html" target="_blank" rel="external">Historický</a>. Oba uzly jsou zase jen další Java procesy spuštěné na serverech. Historický uzel nezpracovává žádná nová data, jen <strong>přebírá segmenty, které vytvořil Realtime uzel.</strong></p>
<p>Koordinátor řídí přiřazování segmentů. Máme-li v clusteru deset Historických uzlů, musíme se rozhodnout, který z nich bude servírovat náš pondělní segment. Koordinátor je ten, kdo má v tomto konečné slovo. Proto Koordinátor sleduje stav clusteru a když zjistí, že přibyl nový segment, nalezne vhodný Historický uzel a přikáže mu, aby si nový segment stáhl z deep storage. Segmenty se nikdy nepřesouvají mezi Druidími uzly přímo, vždy jen přes deep storage.</p>
<p>Pomůžeme si dalším obrázkem:</p>
<div class="figure">
<img src="/images/druid/druidarch.svg" alt="">

</div>
<p>Když Historický uzel stáhne segment, oznámí to clusteru a všechny dotazy na data, která jsou uložena v tomto segmentu, půjdou na tento server. Za normálních okolností platí, že všechny dosud vytvořené segmenty jsou buď na Historických uzlech, nebo na Realtime uzlech.</p>
<p>Časová posloupnost akcí při vytváření segmentu by mohla vypadat takto:</p>
<ul>
<li>úterý 00:00: Realtime uzel začne vytvářet úterní segment. Oznámí clusteru, že všechny dotazy na úterní data mají jít k němu. Dále uzavírá pondělní segment a začíná vytvářet nové indexy pro pondělní segment.</li>
<li>01:30 Kompilace pondělního segmentu dokončena, Realtime uzel začíná nahrávat segment do HDFS.</li>
<li>01:50 Nahrávání dokončeno, Realtime oznamuje clusteru, že vytvořil nový segment.</li>
<li>01:51 Koordinátor přikáže některému Historickému uzlu, aby si k sobě stáhnul pondělní segment.</li>
<li>02:08 Historický uzel dokončil stahování segmentu a informuje cluster o tom, že bude odpovídat na všechny dotazy za pondělek.</li>
<li>02:09 Realtime uzel maže pondělní segment a oznamuje clusteru, že už dále nebude odpovídat na dotazy na pondělní data.</li>
</ul>
<p>Teď ale máme data za pondělí na Historickém uzlu a data za úterý na Realtime uzlu. Co když chceme znát součet impresí za oba dny současně? Musíme se zeptat obou serverů. Naštěstí to ale nemusíme dělat ručně, Druid nám poskytuje čtvrtý typ uzlu, který právě řeší dotazování.</p>
<h2 id="broker-uzel">Broker uzel</h2>
<p>Všechny dotazy posíláme <a href="http://druid.io/docs/0.7.0/Broker.html" target="_blank" rel="external">Brokeru</a>. Broker je další uzel Druidu, který za nás řeší dvě věci:</p>
<ul>
<li>Ptáme-li se na nějaká data, Broker zjistí, které segmenty tato data obsahují a které uzly tyto segmenty servírují.</li>
<li>Broker přepošle dotaz všem uzlům, které servírují potřebné segmenty a čeká na odpovědi. Tyto odpovědi následně spojí do jedné odpovědi (například sečte počty impresí za pondělí a za úterý) a odešle tuto odpověď zpátky klientovi.</li>
</ul>
<p>Broker uzel nám jako klientům zajišťuje transparentnost – nemusí nás zajímat, na kterých serverech jsou uložena naše data, jestli je servíruje Realtime uzel nebo Historický uzel. Broker dále cachuje výsledky z Historických uzlů, čímž se výrazně zlepšuje výkonnost celého clusteru. Historické uzly obsahují immutable segmenty, takže pro dva stejné dotazy vrátí vždy stejné výsledky. Cachovat data z Realtime uzlu si nemůže dovolit, protože tam se data typicky mění každou sekundu.</p>
<div class="figure">
<img src="/images/druid/druidarch2.svg" alt="">

</div>
<h2 id="jak-výpadek-jedné-ze-služeb-ovlivní-chod-clusteru">Jak výpadek jedné ze služeb ovlivní chod clusteru</h2>
<h3 id="výpadky-realtime-uzlu">Výpadky Realtime uzlu</h3>
<p>Na Realtime uzel jsou kladeny největší nároky – musí zpracovávat příchozí zprávy, zároveň je musí poskytovat k dotazování a k tomu všemu musí být nějak odolný vůči výpadkům.</p>
<ul>
<li><p>První přístup byl takový, že si Realtime uzel průběžně ukládal všechna data, která zpracovával, na disk. Po pádu Realtime uzlu a jeho restartu si mohl načíst již zpracovaná data z disku a mohl pokračovat dále tam, kde přestal. Toto ale zdaleka není stoprocentní řešení – mohlo se stát, že se nějaké zprávy zduplikují, ale to zase tak moc nevadí, protože <a href="/lambda-architecture">lambda architektura</a>. Jenomže pokud by shořel celý server, na kterém Realtime uzel běží, tak bychom přišli o všechna data. Navíc ve chvíli, kdy neběží Realtime uzel, třeba i jen minutu, nemáme přístup k datům, které už zpracoval.</p></li>
<li><p>Proto se začaly ručně vytvářet repliky Realtime uzlů. To se bohužel muselo trochu nahackovat přes Kafka consumer group ID – spustily se dva Realtime uzly a každému se nastavilo jiné group ID, viz <a href="/kafka-consumer/">Jak funguje Kafka consumer</a>. Jenomže to způsobilo <a href="https://groups.google.com/forum/#!searchin/druid-development/realtime$20uzel$20kafka$20replication/druid-development/LRFr2Dzw5Po/wAnCtm4FmrAJ" target="_blank" rel="external">další problémy</a> a celkově je to víc hack než řešení. Navíc už není jednoduché jen tak přivézt k životu jednou spadlý Realtime uzel. Máme-li dva Realtime uzly, po pádu jednoho z nich a po jeho znovu nastartování získáme nekonzistentní stav clusteru, protože uzel, který spadl, obsahuje třeba hodinu stará data, zatímco uzel, který celou dobu běžel, má aktuální data.</p></li>
<li><p>To se nakonec vyřešilo tím, že se <a href="https://groups.google.com/forum/#!searchin/druid-development/realtime$20uzel$20kafka$20replica/druid-development/aRMmNHQGdhI/HIB9kRf_6CwJ" target="_blank" rel="external">přešlo od pull-based řešení k push-based řešení</a>. V novějších verzích se proto typicky data do Druidu posílají, místo toho, aby si je Druid sám tahal z Kafky. O tom se ale budeme víc bavit někdy příště. Tento přístup zajistil, že replikace je už snadno konfigurovatelná věc. Můžeme nastavit, že chceme, aby se Realtime uzel třikrát replikoval. Spadne-li jedna replika, automaticky se obnoví až s dalším segmentem – nenastává problém s tím, že bychom získali hodinu stará data.</p></li>
</ul>
<h3 id="výpadky-zbylých-částí-clusteru">Výpadky zbylých částí clusteru</h3>
<ul>
<li><p>Když vypadne jeden <strong>Broker</strong>, můžeme se zeptat druhého.</p></li>
<li><p>Když vypadne <strong>Historický uzel</strong>, převezme jeho práci jiný Historický uzel. V Druidu můžeme nastavit replikaci Historických uzlů, takže pokud ji nastavíme na 2, po výpadku jednoho Historického uzlu se nestane nic – v clusteru bude vždy existovat ještě jeden uzel, který obsahuje dané segmenty. Pokud by vypadly všechny repliky pro daný segment, musel by se segment časem stáhnout na jiný Historický uzel. Do té doby, než by se segment stáhl, by byla data nedotazovatelná, ale zbytek funkčnosti clusteru by to neovlivnilo.</p></li>
<li><p>Když vypadne <strong>Koordinátor</strong> a neexistuje jeho replika, tak se zmrazí stav clusteru. V tuto chvíli není možné, aby se segment z Realtime uzlu dostal na Historický uzel, protože Historickému uzlu nemá kdo říci, aby si stáhl nový segment. Nicméně segment, který zůstal na Realtime uzlu je nadále dotazovatelný. Problém by nastal až ve chvíli, kdy by na Realtime uzlu docházelo místo nebo paměť.</p></li>
<li><p>Když vypadne <strong>MySQL</strong>, tak je to podobné, jako když vypadne Koordinátor.</p></li>
</ul>
<h2 id="horizontální-škálovatelnost">Horizontální škálovatelnost</h2>
<h3 id="realtime-uzel-1">Realtime uzel</h3>
<p>Ve starém přístupu jsme mohli vytvořit tolik <strong>Realtime uzlů</strong>, kolik partitionů obsahoval náš Kafka topic. V novém push-based přístupu už je to zcela nezávislé a můžeme si sami nastavit, kolik Realtime uzlů má zpracovávat vstupní zprávy. Můžeme nastavit, že naše zprávy budou zpracovávány třemi shardy a replikaci můžeme nastavit na dva. Tím dosáhneme toho, že se automaticky vytvoří celkem šest Realtime uzlů, které bude zpracovávat naše data. Vstupní zprávy se rozdělí na třetiny a každá třetina je poslána jiné dvojice uzlů.</p>
<p>Pro příklad předpokládejme, že máme devět zpráv, očíslované od jedné do devíti. Každý ze tří shardů bude zpracovávat “každou třetí zprávu”:</p>
<div class="figure">
<img src="/images/druid/DruidReplikace.svg" alt="">

</div>
<p>Potřebujeme-li se dotázat na aktuální data, musíme se vždy zeptat tří Realtime uzlů. Vypadne-li Realtime #3, zastoupí ho #4. Vypadne-li i ten, jsou zprávy ze Shardu #2 nedostupné. Realtime uzly #3 a #4 by se automaticky znova nastartovaly s novým segmentem (tj. v našem příkladě se začátkem nového dne).</p>
<h3 id="ostatní-uzly">Ostatní uzly</h3>
<ul>
<li><p><strong>Koordinátor</strong> horizontálně škálovatelný není. Máme-li více Koordinátorů, zvolí se mezi nimi leader a ten koordinuje cluster. Koordinátor nicméně vykonává nenáročnou činnost, jeden aktivní uzel by měl bohatě stačit.</p></li>
<li><p><strong>Historické uzly</strong> jsou sobě zcela nezávislé a můžeme jich mít kolik chceme. Každý Historický uzel zkrátka servíruje určitý počet segmentů, které jsou navíc immutable, a Broker ví o tom, který segment je uložený na kterém Historickém serveru. Můžeme mít klidně pro každý segment, tj. pro každá denní data, vlastní Historický uzel.</p></li>
<li><p><strong>Broker</strong> je opět nezávislý uzel, který nepotřebuje vědět o ostatních Brokerech. Broker jen přijímá dotazy a rozesílá je na Historické a Realtime uzly. Můžeme jich mít kolik chceme a v aplikaci pak střídavě rozesílat dotazy na různé Brokery.</p></li>
</ul>
<p>Celkově vzato je Druid velmi horizontálně i vertikálně škálovatelný. Můžeme proto postavit větší cluster, než jaký jsme viděli na posledním obrázku. Pokud bychom nechali počet Realtime uzlů a jen zvětšili počet Historických uzlů, mohl by dotaz „vrať mi součet impresí za uplynulý týden“ položený v neděli dopadnout takto:</p>
<div class="figure">
<img src="/images/druid/DruidCluster.svg" alt="">

</div>
<p>Broker se za nás zeptá Realtime uzlu z každého shardu a zároveň se zeptá všech Historických uzlů, které zrovna obsahují data, která potřebujeme. Počká na všechny odpovědi, ty spojí a vrátí uživateli.</p>
<p>A jaký je <a href="https://groups.google.com/forum/#!topic/druid-user/xPdSmowsQD4" target="_blank" rel="external">dosud největší Druid cluster</a>?</p>
<ul>
<li>50–100 PB surových dat, z kterých Druid po všech kompresích vytvořil 500 TB dat.</li>
<li>20 bilionů událostí (~ řádků).</li>
<li>400 serverů.</li>
</ul>
<h2 id="konzistence-dat">Konzistence dat</h2>
<p><strong>Data nejsou konzistentní napříč Realtime uzly.</strong> Může se totiž stát, že Realtime uzel #1 zpracovává zprávy s offsetem 10 000, zatímco Realtime uzel #2, jeho replika, je trochu pozadu a zpracovává zprávy teprve s offsetem 9 750, tedy je o 250 zpráv pozadu. Tomu se nedá vyhnout, ať už používáme starý přístup, nebo nový přístup. Broker se přitom může jednou zeptat Realtime uzlu #1 a potom uzlu #2. Potom by se mohlo stát, že by v prvním dotazu zjistil od uzlu #1, že už proběhlo 10 000 impresí, ale o sekundu později by z uzlu #2 zjistil, že proběhlo 9 750 impresí, což je zjevně nesmysl, aby o sekundu později nastalo <em>méně impresí</em>.</p>
<p>Nicméně pokud oba Realtime uzly stíhají, tak by tato nekonzistence měla být nepostřehnutelná.</p>
<p>Data napříč <strong>Historickými uzly jsou plně konzistentní</strong>, protože pokud dva Historické uzly servírují segment pro daný časový úsek, tak tyto dva segmenty jsou zcela identické.</p>
<!--
## Více segmentů pro jeden den

Prozatím jsme uvažovali, že když máme segmentGranularity nastavenou na jeden den, že za den vznikne jeden segment = jeden soubor. Ve skutečnosti těch souborů vznikne více. Když máme v běhu dva Realtime uzly, které zpracovávají vždy polovinu dat, tak každý Realtime uzel si vytváří svůj vlastní segment. Tedy každý den by každý Realtime uzel vyprodukoval svůj vlastní segment a tyto segmenty už by se nikde nespojovaly do jednoho velkého segmentu. Je proto možné mít pro jeden den více segmentů.
-->
<h2 id="zookeeper">ZooKeeper</h2>
<p>V předchozích obrázcích jsem například nakreslil šipku z Koordinátoru přímo do Historického uzlu. To nebylo úplně přesné, protože ve skutečnosti Koordinátor s historickým uzlem nekomunikuj přímo, ale pomocí <a href="https://zookeeper.apache.org/" target="_blank" rel="external">ZooKeeperu</a>.</p>
<p>ZooKeeper je hojně používaná služba v distribuovaných systémech, protože umožňuje takové věci jako zvolit leadera v distribuované síti, udržuje synchronizovaný log napříč svými servery a často se používá na ukládání konfigurací či jiných metadat. Kromě toho poskytuje klienty, které jsou schopny reagovat na změnu dat v Zookeeperu.</p>
<p>Potřebuje-li Koordinátor něco říct Historickému uzlu, uloží na předem domluvené místo v ZooKeeperu danou zprávu a Historický uzel si potom sám všimne, že se v ZooKeeperu na dané adrese něco změnilo, zjistí co a provede příslušnou akci.</p>
<p>Ve skutečnosti tak obrázek s komunikací vypadá takhle: (vypůjčeno přímo z <a href="http://druid.io/docs/0.7.0/Design.html" target="_blank" rel="external">dokumentace Druidu</a>)</p>
<div class="figure">
<img src="/images/druid/druid-manage-1.png" alt="">

</div>
<p>Všechny šipky jdou vždy z druidího uzlu do ZooKeeperu, nikdy ne do dalšího druidího uzlu. Na principu se nic nezměnilo. Druiďáci ale mají v plánu <a href="https://groups.google.com/forum/#!topic/druid-development/5s0Uh2NJ15c" target="_blank" rel="external">použití Zookeeperu omezit</a>. Což není úplně neobvyklé, nová Kafka už také daleko méně závisí na ZooKeeperu než starší verze.</p>
<p>Přes ZooKeeper také Realtime a Historické uzly sdělují celému clusteru jaké segmenty jsou u nich aktuálně uložené. Na tyto informace pak reaguje Broker, který musí v každé chvíli vědět, koho se má zrovna zeptat.</p>
<p>Příště si ukážeme, jak Druid agreguje a zpracovává všechny vstupní zprávy, ze kterých vytváří segmenty.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/druid-io-architektura/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Druid.io: distribuovaná immutable databáze pro analytické výpočty]]></title>
      <link>http://programio.havrlant.cz/druid-io/</link>
      <guid>http://programio.havrlant.cz/druid-io/</guid>
      <pubDate>Mon, 27 Apr 2015 16:42:56 GMT</pubDate>
      <description>
      <![CDATA[<p><a href="http://druid.io/" target="_blank" rel="external">Druid.io</a> používáme pro ukládání všech statistických a analytických dat. Je ]]>
      </description>
      <content:encoded><![CDATA[<p><a href="http://druid.io/" target="_blank" rel="external">Druid.io</a> používáme pro ukládání všech statistických a analytických dat. Je to vysoce škálovatelná distribuovaná immutable databáze, která obsahuje přímou podporu pro agregaci dat a pro aproximační algoritmy jako je <a href="/hyperloglog/">hyperloglog</a> nebo histogram.</p>
<h2 id="dlouhá-cesta-k-druidu">Dlouhá cesta k Druidu</h2>
<p>Než se dostaneme k Druidu, podíváme se na to, jak řešíme ukládání statistických dat teď, co jsme plánovali zkusit a proč jsme nakonec zvolili Druidíka. Vše si ukážeme na nejjednodušším příkladu ukládání impresí (tj. zobrazení reklamního banneru). Naše vstupní data vypadají nějak takto:</p>
<table>
<thead>
<tr class="header">
<th align="left">datetime</th>
<th align="left">campaignID</th>
<th align="left">userId</th>
<th align="left">web</th>
<th align="left">browser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-15-03 14:10:54</td>
<td align="left">336</td>
<td align="left">40BBF8</td>
<td align="left">csfd.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 14:10:55</td>
<td align="left">1344</td>
<td align="left">9896AA</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 14:11:01</td>
<td align="left">1344</td>
<td align="left">40BBF8</td>
<td align="left">idnes.cz</td>
<td align="left">Firefox</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 14:11:05</td>
<td align="left">3738</td>
<td align="left">43D03E</td>
<td align="left">csfd.cz</td>
<td align="left">Safari</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 14:11:18</td>
<td align="left">1344</td>
<td align="left">9896AA</td>
<td align="left">idnes.cz</td>
<td align="left">Chrome</td>
</tr>
<tr class="even">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
</tbody>
</table>
<p>Každý řádek reprezentuje jednu impresi. Z těchto surových dat chceme získávat tyto informace:</p>
<ul>
<li>Kolik impresí dosud udělala kampaň XYZ?</li>
<li>Kolik unikátních zobrazení udělala kampaň XYZ za uplynulý týden?</li>
<li>Kolik unikátních zobrazení udělaly všechny kampaně klienta ABC?</li>
</ul>
<p>…a mnoho dalších, toto jsou ty nejzákladnější dotazy. Hlavním problémem samozřejmě je, že těchto dat máme <em>hodně</em>, řekněme zhruba deset tisíc impresí za sekundu, což dělá necelou miliardu denně.</p>
<h3 id="cesta-sql">Cesta SQL</h3>
<p>V současné době máme data v SQL databázi, konkrétně <a href="https://www.infobright.com/" target="_blank" rel="external">InfoBright</a>. Běžné dotazy zvládá odpovídat rychle, nicméně pokud klient požádá o nějaký složitější report, ve kterém se musí hodně joinovat, může si na odpověď počkat i několik dlouhých desítek minut.</p>
<p>Celkově vzato se SQL databáze blbě replikuje a blbě sharduje. Shardováním myslím to, že databáze běží na dvou serverech, v každé databázi je polovina dat a když vy položíte do databáze dotaz, automaticky se provede na obou serverech, obě databáze tento dotaz vyhodnotí a pak nějaký driver tyto dva dílčí výsledky spojí do jednoho. Zkrátka možnost databázi horizontálně škálovat. Proto zatím škálujeme jenom vertikálně.</p>
<p>Jednou z hlavních nevýhod Infobright je navíc to, že je dost drahá.</p>
<h3 id="cesta-nosql-a-předpočítávání-výsledků">Cesta NoSQL a předpočítávání výsledků</h3>
<p>Když jsme tento problém začali řešit, rozhodli jsme se jako první zkusit data předpočítávat. Chtěli jsme vzít nějakou NoSQL Key-value databázi a ukládat do ní data typu:</p>
<ul>
<li>“14. 3. vidělo reklamu XYZ na ČSFD 35 952 lidí, z toho 9744 v Chrome, 4788 ve Firefoxu, 0 v Opeře, …”</li>
<li>“15. 3. vidělo reklamu XYZ na ČSFD 28 434 lidí, z toho 4410 v Chrome, 2772 ve Firefoxu, 0 v Opeře, …”</li>
<li>atd.</li>
</ul>
<p>Vytáhnout počet impresí pro daný den pro daný prohlížeč je pak otázkou vytáhnutí jednoho řádku z databáze. Jenomže tento způsob ukládání trpí několika problémy:</p>
<ul>
<li><p>Ve skutečnosti se <strong>nikdy nebude tahat pouze jeden řádek</strong>. Když budeme chtít znát statistiky za jeden týden, musíme vytáhnout sedm řádků a výsledky sečíst. Navíc chceme podporovat různá časová pásma, což znamená, že bychom předpočítaná data neměli ukládat pod denní granularitou, ale pod hodinovou granularitou. Ale to není velký problém.</p></li>
<li><p>Imprese sice lze sčítat, ale <strong>počet unikátních uživatelů už se sčítat nedá</strong>. Pokud bychom v sobotu měli 1000 unikátů a v neděli 1500 unikátů, neznamená to, že za oba dny tam máme 2500 unikátů. Museli bychom proto předpočítávat počty unikátů pro každý možný časový interval (od 17. 3. do 23. 3 tam bylo 1500 unikátních uživatelů, od 17. 3. do 24. 3. 1750 unikátů atp.) nebo přinejmenším alespoň pro běžné intervaly jako je minulý týden, celý běh kampaně, posledních sedm dní apod. Předpočítávat pro všechny intervaly bude velice náročné, předpočítávat jen některé časy se chvíli zdálo jako docela přijatelné řešení.</p></li>
<li><p><strong>S každou novou dimenzí exponenciálně roste počet klíčů, pro které musíme počítat výsledky</strong>. Pokud bychom zobrazovali reklamu na tisíci webech a ukládali si informaci o deseti prohlížečích, museli bychom si ukládat až 1000 · 10 = 10 000 záznamů za každou hodinu. Pokud bychom dále ukládali i tři hlavní operační systémy, dostaneme se až na 10 000 · 3 = 30 000 záznamů každou hodinu. A tak dále s každou novou dimenzí, podle které bychom chtěli filtrovat. To není dlouhodobě zvládnutelné.</p></li>
</ul>
<h2 id="dude-i-can-store-that-in-memory">Dude, I can store that in memory</h2>
<p>Zajímavé je, že přesně touto cestou si prošli i <a href="http://druid.io/blog/2011/04/30/introducing-druid.html" target="_blank" rel="external">Metamarkeťáci</a>, autoři Druidu, a stejně jako my přišli na to, že ani SQL databáze, ani předpočítávání není ta správná cesta. Tehdy to dopadlo tak, že se nějaký Metamarkeťák podíval na data, která potřebovali prohledávat a pronesl památnou větu: “Dude, I can store that in memory”. Toho se chytli a vznikla první verze Druidu, distribuované databáze, která všechna data, nad kterými se volaly dotazy, držela v paměti. Byl to dobrý nápad? Ano, i ne.</p>
<ul>
<li>Protože jsou všechna data v paměti, jsou dotazy bleskurychlé.</li>
<li>Protože je Druid distribuovaná databáze, nejste omezení velikostí RAMky na jednom počítači. “Přidat” paměť můžete tak, že přidáte nový server do clusteru. Potřebujete-li deset tera paměti, můžete vytvořit cluster ze sto počítačů, každý po sto GB paměti. Druid si s tím poradí.</li>
</ul>
<p>Otázkou zůstává, jestli se to vyplatí. Nebude to moc drahé? Naše současné denní statistiky mají v surové podobě přibližně 50 GB. Za rok by to dalo přibližně 18 TB. Pokud bychom toto množství paměti nakoupili jako <a href="http://aws.amazon.com/ec2/pricing/" target="_blank" rel="external">cloudovou službu u Amazonu</a>, stálo by nás to $210. Za hodinu. Za měsíc by to byly skoro čtyři miliony Kč. (75 instancí r3.8xlarge) No, není to úplně málo.</p>
<h3 id="snižujeme-velikost-dat">Snižujeme velikost dat</h3>
<p>Zkusíme snížit velikost dat.</p>
<ol style="list-style-type: decimal">
<li><p>V těch 50 GB jsou i data, která ve skutečnosti pro statistiky nepotřebujeme – odstraníme je.</p></li>
<li><p>Můžeme nějak rozumně komprimovat data. Místo toho, abychom do databáze ukládali string “chrome”, tak tam uložíme nějakou číselnou konstantu, která tento prohlížeč reprezentuje.</p></li>
<li><p>Blbá jsou IDéčka uživatelů. Pokud chceme spočítat počet unikátních uživatelů, kteří viděli nějakou reklamu, musíme u každé imprese uložit i to, jaký uživatel ji viděl, tj. nějaké jeho unikátní ID. Unikátní ID má ale tu blbou vlastnost, že se velmi špatně komprimuje, protože … no zkrátka protože jsou to příliš unikátní hodnoty. Jak se řeší problém si řekneme v příštím článku, pro teď předpokládejme, že sloupec s IDečky odstraníme.</p></li>
</ol>
<h3 id="agregujeme">Agregujeme</h3>
<p>V tuto chvíli jsme docela slušně zredukovali velikost dat, ale stále ještě nejsme u konce. Můžeme totiž data dále agregovat. Stále máme pro každou impresi jeden řádek, ale to my ani tak moc nepotřebujeme. Potřebujeme vědět, kolik impresí se událo za danou hodinu. Můžeme vzít takovouto tabulku…</p>
<table>
<thead>
<tr class="header">
<th align="left">datetime</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-15-03 10:52:54</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">safari</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:52:58</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">safari</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 10:53:14</td>
<td align="left">3738</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:53:15</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">safari</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 10:53:24</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">chrome</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:53:25</td>
<td align="left">1344</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 10:53:29</td>
<td align="left">3738</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:53:32</td>
<td align="left">3738</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 11:02:38</td>
<td align="left">1344</td>
<td align="left">csfd.cz</td>
<td align="left">safari</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 11:02:43</td>
<td align="left">1344</td>
<td align="left">csfd.cz</td>
<td align="left">safari</td>
</tr>
</tbody>
</table>
<p>…a můžeme ji agregovat na takovouto tabulku:</p>
<table>
<thead>
<tr class="header">
<th align="left">datetime</th>
<th align="left">campaignID</th>
<th align="left">web</th>
<th align="left">browser</th>
<th align="left">impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2015-15-03 10:00:00</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">safari</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:00:00</td>
<td align="left">1344</td>
<td align="left">idnes.cz</td>
<td align="left">chrome</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">2015-15-03 10:00:00</td>
<td align="left">1344</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 10:00:00</td>
<td align="left">3738</td>
<td align="left">csfd.cz</td>
<td align="left">chrome</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
<tr class="even">
<td align="left">2015-15-03 11:00:00</td>
<td align="left">1344</td>
<td align="left">csfd.cz</td>
<td align="left">safari</td>
<td align="left">2</td>
</tr>
</tbody>
</table>
<p>V první tabulce máme co jedno zobrazení reklamy, to jeden řádek. Ve druhé tabulce už jsme si spočítali, že na iDnes.cz byly v Safari v dané hodině a pro danou kampaň tři imprese. Druhá tabulka obsahuje stejně informací jako ta první, s výjimkou časového razítka. Z druhé tabulky už nevyčteme, kolik impresí bylo mezi 10:52:00 a 10:53:00. Ale to nás už moc nezajímá, hodinová granularita nám stačí.</p>
<p>Jak moc se dále data agregují záleží na počtu dimenzí, na jejich kardinalitě, tj. kolik různých hodnot v sloupci máme a na časové granularity – agregace bude větší, když se bude agregovat po dnech než po hodinách. Tato agregace může snížit velikost dat klidně <em>na</em> deset procent, ale také může snížit velikost jen <em>o</em> deset procent; záleží na datech.</p>
<p>A jak jsme si pomohli? Se všemi kompresemi a agregacemi bychom se mohli dostat na řádově nižší velikost dat. Pokud bychom předpokládali, že bychom dokázali data takto zredukovat z 50 GB na 2,5 GB, stál by nás cluster na Amazonu dvacetkrát méně, což znamená 200 000 Kč za měsíc. Ne že by to bylo málo, ale už se o tom dá uvažovat.</p>
<h2 id="dámy-a-pánové-druid.io">Dámy a pánové, Druid.io</h2>
<p>Dobře, už jsme si spočítali, že bychom možná dokázali všechna data vměstnat do paměti a že by to nemuselo být úplně drahé. Co nám tedy <a href="http://druid.io/" target="_blank" rel="external">Druid</a> nabízí, že jsme nakonec skončili u něj?</p>
<p>Druid je distribuovaná databáze napsaná v Javě vhodná pro statistická a analytická data. Není například vhodná pro ukládání textů, takže redakční systém iDnes.cz bych na něm nestavěl. Jaké jsou klíčové vlastnosti?</p>
<ul>
<li><p>Zvládá běžné databázové operace jako jsou filtry nebo grupování; nezvládá join – proto jsou typicky data uložená denormalizovaně. Nad sloupcem umí spočítat počet unikátních hodnot, průměr, medián, jiný percentil, rozložení hodnot. Obecně dokáže nad sloupcem udržovat aproximaci histogramu a z něj spočítat hromadu zajímavých údajů. <strong>Všechno z toho dělá Druid distribuovaně na několika různých serverech.</strong></p></li>
<li><p><strong>Je škálovatelný.</strong> Každá složka Druidu je škálovatelná, pokud už současná velikost clusteru nezvládá zpracovávat dotazy nad daty, snadno lze přidat další stroje, které si k sobě stáhnou část dat a přeberou tak i část dotazů. Každý dotaz se typicky vyhodnocuje na několika Druidích serverech současně. Můžete si to představit tak, že máme například 12 Druidích serverů, na každém jsou data za jeden měsíc roku a dotaz na statistická data za uplynulý půl rok by se paralelně vykonával na šesti serverech.</p></li>
<li><p><strong>Každou část lze replikovat.</strong> Stejně jako třeba u <a href="/kafka">Kafky</a> lze i Druidu jednoduše nastavit replikační faktor. Nastavíme-li ho na dva, budou všechna data uložena na dvou odlišných strojích. Pokud jeden ze serverů vypadne, dotazy se přesměrují na druhý stroj.</p></li>
<li><p><strong>Je realtime.</strong> Druid zvládá indexovat desetitisíce nově příchozích zpráv za sekundu a tyto události dává okamžitě k dispozici pro dotazování.</p></li>
<li><p><strong>Všechna data jsou immutable.</strong> Druid v podstatě zvládá pouze <code>append</code> dat. Neumí žádný <code>update</code>. Každá zpráva, kterou appendujeme, musí mít časové razítko, aby šlo určit, do kterého časového intervalu patří. Do Druidu proto patří <em>události</em>, tj. data, která nastala v nějaký okamžik a tato data jsou na vždy platná. Ve chvíli, kdy Druid zpracuje <em>včerejší</em> data, se už tato data nemůžou nijak změnit. Potřebujete-li změnit stará data, není jiné cesty než stará data smazat a vše znova přepočítat (což se běžně dělá).</p></li>
</ul>
<p>To jsou asi věci, které jsme všichni čekali, když se náš seriál zabývá budováním statistik se zaměřením na škálovatelnost, odolnost vůči výpadkům a na nízkou latenci. Co dále Druid umí? Všecho, o čem jsme se bavili v předchozí kapitole:</p>
<ul>
<li><p><strong>Druid používá různé kompresní mechanismy pro zmenšení velikost dat, která do něj vkládáme.</strong> Například i slovníky, takže můžeme do Druidu valit data jako je “chrome” a Druid se už sám postará o to, aby se daného sloupce uložila nějaká číselná konstanta.</p></li>
<li><p><strong>Druid zvládá agregovat data postupem, který jsme si ukázali výše.</strong> Stačí mu nastavit, které sloupce má jak agregovat a nastavit časovou granularitu. Pokud Druid najde v dané hodině dva stejné řádky, spojí je do jednoho a uloží k němu informaci o tom, že tento řádek reprezentuje dvě imprese. O agregaci budeme více mluvit příště.</p></li>
<li><p><strong>Druid má přímou podporu pro <a href="/hyperloglog/">HyperLogLog</a></strong>, díky čemu se můžeme efektivně zbavit sloupce s IDečkami uživatelů. Díky Druidovi jsme schopni velmi přesně odhadnout počet unikátních uživatelů, kterým se zobrazila nějaká reklama bez toho, aniž bychom fakticky měli uloženo, komu se jaká reklama zobrazila. Díky <a href="/sjednoceni-hyperloglogu/">sjednocení HyperLogLogu</a> bude fungovat i agregace.</p></li>
<li><p><strong>Druid je schopný udržovat již zmiňovanou <a href="http://en.wikipedia.org/wiki/Histogram" target="_blank" rel="external">aproximaci histogramu</a> pro daný sloupec</strong>.</p></li>
</ul>
<h2 id="dobře-je-to-všechno-hezké-ale-fakt-musí-být-vše-v-paměti">Dobře, je to všechno hezké, ale … fakt musí být vše v paměti?</h2>
<p>Nemusí! I kluci z Metamarkets si asi časem uvědomili tři věci:</p>
<ul>
<li>Když už spotřebovaná paměť leze do jednotek TB, tak už tak moc levná není.</li>
<li>SSD vlastně nejsou tak úplně pomalé.</li>
<li>Opravdu je nutné držet v paměti tři roky stará data jenom proto, abychom je byli schopni dotázat jednou za uherský rok?</li>
</ul>
<p><strong>Současná verze Druidu proto umožňuje dotazovat i ta data, která jsou uložená na disku a nejsou přímo načtená v paměti.</strong> Je to samozřejmě pomalejší, ale pořád je to dostatečně rychlé. Zároveň umožňuje definovat pravidla, kam se mají jaká data ukládat. Takže můžeme vytvořit pravidla, že data za poslední tři měsíce budou na nejrychlejších strojích s hromadou paměti, zatímco starší data budou na horších strojích s méně paměti. Idea je, že v 90 % případů stejně taháme <em>nová</em> data, ne data několika let stará.</p>
<p>Není to nijak zvlášť nový přístup, takový <a href="https://blog.twitter.com/2014/building-a-complete-tweet-index" target="_blank" rel="external">Twitter používá něco velmi podobného</a>. Aktuální tweety udržuje v paměti, zatímco staré tweety má uložené na SSD (a ještě k tomu trošku vyladili kernel, ale kdo z nás to občas nedělá, že?).</p>
<h2 id="příště">Příště…</h2>
<p>Příště se podíváme na architekturu Druidu.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/druid-io/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Uložení lokálního stavu v Samze]]></title>
      <link>http://programio.havrlant.cz/samza-local-state/</link>
      <guid>http://programio.havrlant.cz/samza-local-state/</guid>
      <pubDate>Mon, 20 Apr 2015 16:54:07 GMT</pubDate>
      <description>
      <![CDATA[<p>V minulé části série jsme si představili problém: pokud restartujeme Samza job, nevyhnutelně přijdeme o lokální stav aplikace, přijdeme o]]>
      </description>
      <content:encoded><![CDATA[<p>V minulé části série jsme si představili problém: pokud restartujeme Samza job, nevyhnutelně přijdeme o lokální stav aplikace, přijdeme o všechna data, které jsme měli uloženy pouze v paměti počítače. Jak nám Samza framework pomůže tento problém vyřešit?</p>
<p>Ukážeme si to na jiném Samza jobu. Jednou z dalších věcí, které měříme, jsou pochopitelně kliky na reklamní bannery. Zpráva o kliku také nakonec skončí v <a href="/kafka">Kafce</a> v topicu <em>clicks</em> a ten se dále zpracovává v Samze. Představme si nyní, že nějaký uživatel přijde na ČSFD, koukne na reklamu a … no to není možné! Vyhrál iPhone! Rychle kliká, stránka se ale neotevírá. Stále jako zběsilý kliká na reklamní banner, až se nakonec dostane na cílovou stránku a začne skákat radostí.</p>
<p>Jaká je největší zrada celého příběhu? Pokud uživatel desetkrát za sebou kliknul na reklamní banner, my to započítáme jako deset různých kliků, přitom logicky by to měl být spíše jeden klik. Abychom to byli schopni takto spočítat, máme v zásadě dvě možnosti: zařídit, aby se po druhém kliku na banner už neodeslal požadavek na naše servery, nebo musíme všechny požadavky ještě dále filtrovat a odstraňovat duplicity.</p>
<p>Z různých důvodů nemusí být úplně jednoduché zařídit, aby se už pdoruhé neodeslal požadavek na naše servery, takže zvolíme druhou možnost. Potřebujeme napsat Samza job, který bude číst topic <em>clicks</em> a bude odstraňovat duplikované zprávy, které bude odesílat do topicu <em>unique_clicks</em>.</p>
<h2 id="naivní-verze">Naivní verze</h2>
<p>Každá naše zpráva má unikátní ID. Pokud budou v topicu dvě shodné zprávy, budou mít také shodné ID. Nemusíme si uchovávat obsah každé zprávy, stačí nám si pamatovat IDéčka těch zpráv, které jsme už viděli.</p>
<p>Napíšeme Samza job tak, že bude číst zprávy z topicu <em>clicks</em> a IDéčko každé dosud nepřečtené zprávy si uloží do nějaké množiny. Pokud z topicu přečteme znova stejnou zprávu, budeme mít ID této zprávy uložené v množině, takže tuto zprávu zahodíme; nebo ji pošleme do topicu <em>duplicated_clicks</em>, abychom o tuto informaci nepřišli.</p>
<p>Spustíme Samza job.</p>
<p>Za tři hodiny spadne na nedostatku paměti.</p>
<h2 id="přidáme-windowing">Přidáme windowing</h2>
<p>Problém byl v tom, že jsme do množiny neustále přidávali nová a nová IDéčka, ale neodstraňovali jsme stará. Nevyhnutelně muselo dojít k tomu, že dojde paměť. Tento problém lze vyřešit například <a href="/samza-windowing">windowingem</a>, tj. budeme z paměti odstraňovat IDéčka, která jsme uložili před více než hodinou a budeme doufat, že to bude stačit.</p>
<p>Máme ale větší problém. Po restartu aplikace přijdeme a celý lokální stav aplikace a tím pádem i o seznam již přečtených zpráv. Po nastartování Samza jobu nutně začneme propouštět i ty zprávy, které jsme přečetli v minulém běhu programu.</p>
<div class="figure">
<img src="/images/samza/samzalocalstate1.svg" alt="">

</div>
<p>Zatímco druhou zprávu s ID 46 vyfiltrujeme, protože ji máme uloženou v množině, druhá zpráva s ID 47 nám projde, protože to bude první zpráva, kterou přečteme po startu Samza jobu a naše množina dosud přečtených zpráv je aktuálně prázdná.</p>
<p>Potřebovali bychom zkrátka zařídit, aby data, která máme v paměti, přežila pád aplikace. Podobný problém jsme už řešili, když jsme se bavili o <a href="/kafka-consumer">ukládání offsetů v Kafce</a>. Hlavním rozdílem oproti offsetům je ten, že lokální stav aplikace může být mnohem větší. Potřebujeme nějaký prostor na uložení potenciálně velkého množství dat, který přežije pád našeho Samza jobu. Jaké máme možnosti?</p>
<ul>
<li>Můžeme data ukládat do souboru. No. Tam by se asi blbě prohledávala, že?</li>
<li>Dobře, můžeme data držet v paměti, kde je budeme prohledávat, a do souboru je budeme pouze zálohovat. Problémem nastane, když velikost dat přeroste dostupnou paměť.</li>
<li>Můžeme data ukládat do nějaké lokální databáze. Jenomže po restartu může Samza job běžet na úplně jiném počítači a k lokálně uloženým datům vůbec nebude mít přístup.</li>
<li>Můžeme data ukládat do nějaké vzdálené databáze na jiném stroji. To už je lepší cesta, ale asi tím slušně snížíme rychlost celého Samza jobu.</li>
</ul>
<h2 id="keyvalue-store">KeyValue store</h2>
<p>Samza nám nabízí něco ještě jiného. Ideálně bychom chtěli mít všechny data v paměti, protože to je nejrychlejší. Druhý nejlepší způsob je mít data ne v paměti, ale alespoň na stejném počítači.</p>
<p>V Samze můžeme použít vestavěnou KeyValue store, která je uložena lokálně na disku, přičemž každá změna, kterou v KeyValue store provedeme, se automaticky zálohuje v Kafce. Samza si sama vytvoří v Kafce nový topic, který pojmenuje podle jobu, který je zrovna spuštěn a do toho topicu bude ukládat všechny změny, které provedeme v KeyValue store.</p>
<p>Když chceme změnit hodnotu v KeyValue storu, uděláme něco jako</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.put(<span class="string">"Star trek"</span>, <span class="number">47</span>);</span><br></pre></td></tr></table></figure>
<p>Samza zařídí, aby se v KeyValue store přepsala hodnota pod klíčem “Star trek” hodnotou 47 a zároveň vypálí do správného kafka topicu zprávu v přibližném tvaru</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;"key": "Star trek", value: 47&#125;</span><br></pre></td></tr></table></figure>
<p>Tím získáme historii všech změn, které byly ve storu provedeny.</p>
<p>Po restartu Samza jobu se jako první vytvoří nová prázdná KeyValue store, přečte se celý obsah záložního topicu a aplikují se na KeyValue store všechny přečtené změny. Tím dostaneme KeyValue store do stavu, v jakém byl, když jsme restartovali Samzu.</p>
<p>Kafka nám přitom přímo poskytuje podporu pro takovéto ukládání zpráv – každé zprávě totiž lze přiřadit nějaký <em>klíč</em>. V Kafce můžeme dále zapnout něco, co se jmenuje <a href="https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction" target="_blank" rel="external">Log Compaction</a>. Tato feature bude v pravidelných intervalech hledat zprávy v topicu se stejným klíčem. Protože nemá smysl mít v topicu více zpráv se stejným klíčem – zajímá nás vždy jen ta nejnovější hodnota – Kafka automaticky smaže všechny starší zprávy.</p>
<div class="figure">
<img src="/images/samza/logcompaction1.svg" alt="">

</div>
<p>Pro každý klíč zůstala v topicu (resp. partitioně) pouze jedna zpráva a to ta nejnovější. Offsety se nezmění. Z tohoto důvodu můžou v jedné partitioně v Kafce vzniknout “díry”, ve kterých chybí zprávy s určitým offsetem. Bohužel to také znamená, že každá zpráva musí v sobě obsahovat svůj offset, což mírně komplikuje kód celé Kafky, ale to už není náš problém.</p>
<p>Díky Log Compaction nebude záložní topic růst do nekonečna, ale bude vždy přibližně stejně velký jako KeyValue store samotná. Budeme-li měnit jen sto hodnot v KeyValue store, v topicu po promazání bude také jen sto zpráv. Přečteme-li nyní celý záložní topic, získáme stav KeyValue storu.</p>
<p>Náš Samza job, který má deduplikovat topic s kliky bychom mohli napsat tak, že by všechna IDéčka ukládal do KeyValue storu a při kontrole by se zase do KeyValue díval. Po restartu aplikace by se KeyValue obnovil do původního stavu, ať spustíme Samza job na kterémkoliv počítači.</p>
<p>Pro přesné použití se podívejte do dokumentace <a href="http://samza.apache.org/learn/documentation/0.7.0/container/state-management.html" target="_blank" rel="external">State Managmentu</a>. Samza aktuálně jako KeyValue store využívá <a href="https://github.com/google/leveldb" target="_blank" rel="external">levelDB</a>. V budoucích verzích by měla přibýt i <a href="http://mail-archives.apache.org/mod_mbox/samza-commits/201502.mbox/%3CJIRA.12771866.1422917062000.234561.1422917317497@Atlassian.JIRA%3E" target="_blank" rel="external">čistě in-memory KeyValue store</a> – to pokud víte, že potřebujete ukládat relativně málo dat, které se vám jistě vlezou do paměti, ale stále byste je chtěli mít zálohované v Kafce.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/samza-local-state/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Windowing v Samze]]></title>
      <link>http://programio.havrlant.cz/samza-windowing/</link>
      <guid>http://programio.havrlant.cz/samza-windowing/</guid>
      <pubDate>Mon, 13 Apr 2015 16:15:30 GMT</pubDate>
      <description>
      <![CDATA[<p>Ukážeme si, jak implementovat úlohy, které jsou nějak závislé na čase. Například průměr hodnot za posledních třicet sekund a podobně. Min]]>
      </description>
      <content:encoded><![CDATA[<p>Ukážeme si, jak implementovat úlohy, které jsou nějak závislé na čase. Například průměr hodnot za posledních třicet sekund a podobně. Minule jsme skončili u toho, že máme dva topicy: <em>bidrequests</em> a <em>bidresponses</em>. Další částí skládačky je, že chceme ke každému požadavku najít odpovídající odpověď a spojit tyto dvě zprávy do jedné, abychom ve výsledné databázi měli jeden řádek, ve kterém budeme mít všechna potřebná data – jak z požadavku, tak z odpovědi.</p>
<p>Zprávy budeme párovat podle <em>requestId</em>, odpovídající požadavky a odpovědi ho mají stejné. Ukážeme si několik různých postupů, od nejvíce naivních, po propracované. Náš Samza job bude číst oba topicy, můžeme předpokládat, že máme jen jednu partitionu, pro následující algoritmy to nebude nijak důležité. Samza čte oba topicy stejnou rychlostí. Výstupní zprávě, která obsahuje data jak z požadavku, tak z odpovědi, budeme říkat <em>aukce</em>. Všechny tyto aukce pošleme do Kafka topicu <em>bidauctions</em>.</p>
<h2 id="prostě-to-přečti-a-spoj">Prostě to přečti a spoj</h2>
<p>První naivní algoritmus vypadá takto:</p>
<ol style="list-style-type: decimal">
<li>Máme-li na vstupu požadavek, uložíme si ho do nějaké lokální cache.</li>
<li>Máme-li na vstupu odpověď, nalezneme v cachi odpovídající požadavek, spojíme je a odešleme aukci do Kafky.</li>
</ol>
<p>To vypadá příliš jednoduše na to, aby to fungovalo. Problém je, že se může stát, že nám odpověď přijde do Samzy dříve než požadavek. My sice máme zaručené pořadí zpráv v topicu (resp. v partitioně), takže požadavek, který jsme odpálili dříve taky dříve přečteme, ale nemáme zaručeno pořadí zpráv napříč dvěma topicy.</p>
<p>Ve hře je přitom příliš mnoho faktorů, které mohou ovlivnit rychlost přenosu zpráv. Třeba se mohlo stát, že nastala nějaká porucha sítě a všechny požadavky v topicu bidrequests se kvůli tomu o pět sekund zpozdily. V takovou chvíli bychom četli z druhého topicu odpovědi, ke kterým jsme ještě nepřečetli požadavky.</p>
<p>Náš algoritmus nefunguje.</p>
<h2 id="dobře-budeme-čekat-navzájem">Dobře, budeme čekat navzájem</h2>
<p>Upravíme náš algoritmus, aby v případě, kdy nenalezne požadavek k odpovědi tuto odpověď uložil:</p>
<ol style="list-style-type: decimal">
<li>Máme-li na vstupu požadavek, podíváme se, jestli už jsme nepřečetli odpověď se stejným requestId. Pokud ano, spojíme je a odešleme. Pokud ne, uložíme požadavek do cache.</li>
<li>Totéž pro odpověď. Máme-li odpovídající požadavek, odešleme, jinak odpověď uložíme do cache.</li>
</ol>
<p>Tím jsme zaručili, že nezáleží na pořadí, v jakém přečteme odpovědi nebo požadavky. Ať už přečteme odpověď dříve než požadavek nebo naopak, náš algoritmus bude fungovat.</p>
<p>Co když ale odpověď nemáme? Nikde není psáno, že pro každý požadavek nám odpověď přijde; občas zkrátka nepřijde, ať už je důvod jakýkoliv. V takovém případě nám bude v paměti viset uložený požadavek, ke kterému nikdy nenajdeme odpověď. Časem náš job selže na nedostatku paměti.</p>
<p>Náš algoritmus nefunguje.</p>
<h2 id="přidáme-expiraci">Přidáme expiraci</h2>
<p>Chtělo by to nějaký mechanismus, který by nám umožnil odstranit zprávy, které už máme v cachi uložené příliš dlouho. K tomu nám může pomoci Samza a její metoda <code>window</code>. Do configu přidáme</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">task.window.ms=1000</span><br></pre></td></tr></table></figure>
<p>a dále v našem StreamTasku implementujeme rozhraní <a href="http://samza.apache.org/learn/documentation/0.7.0/api/javadocs/org/apache/samza/task/WindowableTask.html" target="_blank" rel="external">WindowableTask</a>, tedy přidáme metodu <code>window</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">window</span><span class="params">(MessageCollector collector, TaskCoordinator coordinator)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Při ukládání každé zprávy do cache si zároveň poznamenáme čas, kdy jsme zprávu přečetli. Samza se nyní postará o to, aby každou sekundu (každých <code>task.window.ms</code> milisekund) zavolala naši metodu <code>window</code> a to ve stejném vlákně, jako se volá metoda <code>process</code>. Výhoda je jasná – nemusíme si dělat starosti s race condition a jinými srandičkami, pokud by se metoda <code>window</code> volala v jiném vlákně.</p>
<p>V metodě <code>window</code> projdeme cache a najdeme ty zprávy, které byly uložené před více než minutou (třeba). Tyto zprávy pak odešleme do Kafky bez svého protějšku.</p>
<p>Samza sice má extra podporu pro generování metrik, ale kdyby neměla, mohli bychom si v metodě <code>window</code> implementovat vlastní metriky – například bychom mohli každou sekundu někam posílat, kolik jsme přečetli zpráv z daného topicu atp.</p>
<p>To už je docela dobré řešení a bude více méně fungovat. Jisté mouchy ale stále ještě má.</p>
<h2 id="nestíhačka">Nestíhačka</h2>
<p>Přidejme provoz. Dejme tomu, že náš Samza job zvládá číst 2000 zpráv za sekundu. Protože čte stejně rychle z obou topiců, znamená to, že přečte tisíc požadavků a tisíc odpovědí za sekundu. Je to správně? Abychom si lépe nasimulovali co se děje, předpokládejme, že nám chodí poměrně málo odpovědí, například, že pro celých 90 % požadavků nepřijde žádná odpověď.</p>
<p>Pro ilustraci mějme v topicu jeden milion nezpracovaných požadavků a sto tisíc nezpracovaných odpovědí. Žádné další zprávy do topiců nechodí. Protože Samza čte z obou topiců rychlostí 1000 zpráv za sekundu, přečte všechny odpovědi za 100 sekund. Za jak dlouho přečte všechny požadavky? (Běloun, strana 168, cvičení 34)</p>
<p>Za prvních 100 sekund přečetla Samza 100 000 zpráv. Zbylých 900 000 zpráv už bude číst rychlostí 2000 zpráv za sekundu, protože už nečte žádné odpovědi. Přečte je za 450 sekund. Celkem čte požadavky 550 sekund. Jednoduchou animací bychom mohli rychlost čtení znázornit takto:</p>
<div id="rychlostcteni1" style="position: relative">
<div id="progressbar1" style="position: absolute; top: -10px; left:0; height: 90px; background-color: #9E0303">
 
</div>
<div style="height:30px; background-color: #0F520D; margin-top:10px; text-align: center; color: white; font-weight: bold; line-height: 30px">
bidrequests
</div>
<div style="height:30px; background-color: #0E2978; width: 20%; margin-top:10px; text-align: center; color: white; font-weight: bold; line-height: 30px">
bidresponses
</div>
</div>
<script type="text/javascript">
window.onload = function() {
    var paragraphWidth = document.getElementById("rychlostcteni1").offsetWidth; 
    
}
</script>
<p>V předchozí kapitole jsme si přitom řekli, že pokud máme v paměti nějakou zprávu uloženou déle než jednu minutu, tak ji zahodíme/pošleme bez zpárované zprávy. Samza job tedy po 160 sekundách (100 sekund načítání + 60 sekund doba expirace) jistě nebude mít v paměti žádnou uloženou odpověď, přitom ještě dalších 390 sekund bude číst požadavky, ke kterým už ale nenalezne žádnou odpověď – všechny jsme už zahodili.</p>
<p><strong>Protože máme odpovědí desetkrát méně než požadavků, měli bychom je také číst desetkrát pomaleji</strong>, jinak nebudeme číst odpovídající požadavky a odpovědi v přibližně stejné době. V opačném případě dojde k tomu, že se nám nepodaří zpárovat velkou část požadavků a odpovědí. Zkrátka chceme topicy číst takto:</p>
<div id="rychlostcteni2" style="position: relative">
<div id="progressbar2" style="position: absolute; top: -3px; left:0; height: 36px; background-color: #9E0303">
 
</div>
<div id="progressbar3" style="position: absolute; top: 37px; left:0; height: 36px; background-color: #9E0303">
 
</div>
<div style="height:30px; background-color: #0F520D; margin-top:10px; text-align: center; color: white; font-weight: bold; line-height: 30px">
bidrequests
</div>
<div id="bidresponses" style="height:30px; background-color: #0E2978; width: 20%; margin-top:10px; text-align: center; color: white; font-weight: bold; line-height: 30px">
bidresponses
</div>
</div>
<script type="text/javascript">
window.onload = function() {
    var paragraphWidth = document.getElementById("rychlostcteni2").offsetWidth; 
    var progressbar2Left = 0;
    var progressbar3Left = 0;
    var progressbar2 = document.getElementById("progressbar2");
    var progressbar3 = document.getElementById("progressbar3");
    var progressbar1Left = 0;
    var progressbar1 = document.getElementById("progressbar1");
    var bidresponsesWidth = document.getElementById("bidresponses").offsetWidth;

    setInterval(function() {
        progressbar1.style.left = progressbar1Left + "px";
        progressbar1Left = (progressbar1Left + 3) % paragraphWidth;
        progressbar2.style.left = progressbar2Left + "px";
        progressbar2Left = (progressbar2Left + 3) % paragraphWidth;
        progressbar3.style.left = progressbar3Left + "px";
        progressbar3Left = (progressbar3Left + 0.6) % bidresponsesWidth;
    }, 25);
}
</script>
<p>Tento problém se typicky projeví jen ve chvíli, kdy Samza nestíhá zpracovávat zprávy z Kafky a zprávy se začnou v Kafce hromadit. Pokud by v Kafce přibylo jen tisíc požadavků a sto odpovědí za sekundu, tak by se tento problém neprojevil. Případně stačí, když je Samza deset minut mimo provoz a po nastartování musí dohánět staré zprávy.</p>
<p>Jaké je řešení? Určitě nezvyšovat dobu expirace, tím se ten problém jen posune. Samza nám poskytuje možnost ovlivnit, z kterého topicu se bude zrovna číst další zpráva: <a href="https://wiki.apache.org/samza/Pluggable%20MessageChooser" target="_blank" rel="external">MessageChooser</a>. Díky němu můžeme napsat náš Job tak, abychom například vždy přečetli deset zpráv z topicu <em>bidrequests</em> a pak jednu zprávu z topicu <em>bidresponses</em>.</p>
<p>To ale není dobré řešení, protože když se výrazně změní podíl odpovědí, přestane takový algoritmus fungovat.</p>
<p>My jsme tento problém vyřešili tak, že každá zpráva obsahuje i časové razítka s dobou vzniku a náš MessageChooser si vždy prohlédne nabízené zprávy z obou topiců a vybereme si tu, která je nejstarší. Tím přirozeně docílíme vyrovnaného čtení z obou topiců. Kód v MessageChooserovi se provádí ještě před metodou <code>process</code>, naše logika v této metodě může zůstat nezměněna.</p>
<h2 id="restart-aplikace">Restart aplikace</h2>
<p>Teď už vše musí fungovat, že? Samozřejmě, že ne. Následující obrázek zachycuje, co se stane, když v jeden okamžik restartujeme Samza job. Ta v tu chvíli čte souběžně dva topicy, požadavky a odpovědi.</p>
<div class="figure">
<img src="/images/samza/restartsamza.svg" alt="">

</div>
<p>Request 5 se má párovat s Response 5, pochopitelně. Jenomže po restartu jako první přečteme odpověď 6, přitom požadavek 6 jsme přečetli už před restartem. Odpověď 6 už nikdy nespárujeme s požadavkem 6, protože ho po restartu nikdy nenačteme. Podobný problém bude mít předchozí běh Samzy – před restartem zase přečteme požadavek 7, ale odpověď 7 přečteme až po restartu.</p>
<p>Aby k tomuto problému nedocházelo, je nutné si přenést část lokálního stavu aplikace do dalšího běhu. Jak na si ukážeme příště.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/samza-windowing/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Samza: distributed stream processing framework]]></title>
      <link>http://programio.havrlant.cz/samza/</link>
      <guid>http://programio.havrlant.cz/samza/</guid>
      <pubDate>Mon, 06 Apr 2015 14:15:34 GMT</pubDate>
      <description>
      <![CDATA[<p>V předchozích částech jsme se bavili o <a href="/kafka">Kafce</a>, což je aplikace, která slouží k přenosu zpráv z jednoho serveru na jin]]>
      </description>
      <content:encoded><![CDATA[<p>V předchozích částech jsme se bavili o <a href="/kafka">Kafce</a>, což je aplikace, která slouží k přenosu zpráv z jednoho serveru na jiný. <a href="http://samza.apache.org/" target="_blank" rel="external">Samza</a> je Javový framework, který nám pomáhá tyto protékající zprávy dále zpracovávat.</p>
<h2 id="co-děláme-se-zprávami-před-uložením-do-databáze">Co děláme se zprávami před uložením do databáze</h2>
<p>Jaké zpracování mám na mysli? Vraťme se k úkolu, který jsem popisoval v předchozích částech. Provozujeme burzu s reklamními pozicemi. Pro každou reklamní pozici pošleme několika serverům, DSPéčkům, nabídku (bid request) a očekáváme odpověď (bid response). Tato data ukládáme do Kafka topicu. (<a href="/kafka/">detailněji jsem to popsal minule</a>) Co s těmi daty dále děláme?</p>
<ul>
<li><p>Požadavky a odpovědi jsou <a href="https://code.google.com/p/openrtb/wiki/OpenRTB_Examples" target="_blank" rel="external">vcelku nehezké JSONy</a>. My na ukládání používáme Druid.io, která nepodporuje ukládání takto složitých JSONů, takže potřebujeme něco, co nám zanořený JSON “zlinearizuje” na jeden klasický řádek o x sloupcích.</p></li>
<li><p><em>bidrequests</em> a <em>bidresponses</em> jsou dva nezávislé topicy, v jednom jsou požadavky a v druhém odpovědi. Do databáze ale chceme ukládat jeden záznam, který bude uchovávat jak data z požadavku, tak data z odpovědi, tj. chceme ukládat něco jako <code>merge(request, response)</code>. Každý požadavek jde s každou odpovědí spárovat podle <em>requestID</em>. Potřebujeme nějaký program, který bude číst současně oba topicy a pro každou odpověď nalezne původní požadavek na základě <em>requestID</em>, tyto JSONy nějak inteligentně spojí a pošle zase dál na zpracování.</p></li>
<li><p>V jiné části systému do Kafky ukládáme informace o tom, když někdo klikne na banner. Pokud na něj někdo klikne dvakrát rychle za sebou, vyšlou se na naše servery dva požadavky o kliku, které budou úplně stejné. V Kafka topicu budou dvě shodné zprávy, které ale nechceme ukládat do databáze. Potřebujeme program, který z topicu vyháže duplikace.</p></li>
<li><p>Když někdo klikne na reklamu, máme přímo v GET požadavku informaci o tom, na jakou reklamu klikl. Nemáme ale například informaci o tom, jakému klientovi tato reklama patří. Přitom je to informace, kterou bychom rádi měli uloženou přímo se zprávou o kliku. Musíme tak mít nějaký program, který se ještě před uložením zeptá nějaké databáze, který klient vlastní reklamu s tímto ID a tuto informaci musí přidat do zprávy.</p></li>
</ul>
<p>Toto jsou asi čtyři nejčastější případy využití nějaké (pre)processing aplikace: transformace dat, join dat, filtrace dat a obohacení dat. Samza nám s tímto preprocessingem velmi pomáhá.</p>
<h2 id="proč-vůbec-používat-nějaký-framework">Proč vůbec používat nějaký framework?</h2>
<p>Všechny zprávy nám tečou v Kafce. Co nám brání napsat si Javovou (Python, Ruby, Node.JS, …) aplikaci, která bude číst zprávy z Kafky, něco s nimi udělá a pak je zase uloží zpět do Kafky nebo jinam? Samozřejmě, že to jde, ale museli bychom řešit problémy, které už jsou v Samze vyřešené. Například:</p>
<ul>
<li><p>Co se stane, když z nějakého důvodu naše aplikace spadne? Samza se automaticky postará o znovunastartování aplikace.</p></li>
<li><p>Jak moc škálovatelná naše aplikace bude? V Samze stačí přepsat jedno číslo v configu a rázem neběží na jednom serveru, ale na deseti.</p></li>
<li><p>Často je potřeba <em>windowing</em>, který využíváme u párování požadavků a odpovědí. Obecně jde o to, jak provést nějakou akci každých deset sekund nebo jak přidat našim datům nějaké TTL – třeba když přečteme z topicu nějaký bid request, tak na odpovídající bid response čekáme jen určitou dobu. Dalším příkladem může být plovoucí průměr nějakých dat za posledních x sekund atp.</p>
<p>Samza nám pomáhá tím, že má vlastní <a href="http://samza.apache.org/learn/documentation/0.8/container/event-loop.html" target="_blank" rel="external">event loop</a> a poskytuje nám dvě základní metody: <code>process</code> a <code>window</code>. Metoda <code>process</code> se zavolá, když přijde nová zpráva, metoda <code>window</code> se zavolá každých x sekund (konfigurovatelné). Výhodou je, že obě metody se vykonávají ve stejném vlákně (podobně jako v Node.JS), takže nehrozí žádné kolize kvůli více vláknům a podobně. Přitom toto primitivum nám stačí na implementaci všech předchozích scénářů.</p></li>
<li><p>Vezměme si příklad s filtrováním duplikovaných zpráv. Abychom byli schopni odstraňovat duplikované zprávy, musíme si nutně <em>někde</em> pamatovat ty zprávy, které už jsme zpracovali (nebo alespoň nějaká IDéčka, případně jejich hashe apod.). Když si tyto zprávy budeme ukládat do paměti, přijdeme o ně ve chvíli, kdy náš program spadne. Když si zprávy budeme ukládat do nějaké vzdálené databáze, bude to nejspíš pomalé. Samza to řeší tak, že si data lze uložit lokálně do key-value databáze, ta může být i in-memory, a každá změna v databázi je zálohována mimo server, na kterém běží Samza. Při restartu Samzy lze všechna data uložená do key-value databáze zpětně získat.</p></li>
</ul>
<p>Toto všechno jsou problémy, na které bychom rychle narazili, kdybychom takový <em>preprocessing</em> zpráv psali sami od píky. Samza je řeší za nás, proto ji používáme. Samza dělá <a href="http://samza.apache.org/learn/documentation/0.7.0/comparisons/spark-streaming.html" target="_blank" rel="external">něco podobného jako Apache Spark</a>, pokud znáte.</p>
<h2 id="jak-samza-funguje">Jak Samza funguje</h2>
<p>Samza je framework napsaný v Javě, který se spouští nad Hadoopem; využívá YARN. K čemu je to dobré? Hadoop cluster je hodně stručně řečeno hromada počítačů, na kterých běží Hadoop aplikace, která monitoruje, jaké procesy na daném stroji běží a kolik prostředků zabírají, případně kolik prostředků si procesy alokovaly. Součástí Hadoopu je i HDFS, distribuovaný file system, ale ten Samza téměř nevyužívá.</p>
<p>Pokud chceme v Hadoopu spustit nějakou novou aplikaci, řekneme to YARNu, což je část Hadoopu. Zároveň s tím řekneme, kolik prostředků budeme pro tu aplikaci potřebovat (2 GB RAM, 1 CPU, například). YARN se podívá, jestli je na nějakém počítači tolik volných prostředků a pokud ano, spustí tam danou aplikaci. My jako uživatelé komunikujeme pouze s YARNem a nestaráme se o to, na kterém počítači se nakonec naše aplikace spustí. YARN zároveň hlídá, jestli naše aplikace běží – pokud spadne, automaticky ji spustí znova.</p>
<p>Pro nás je důležité, že se nemusíme starat o distribuci naší Samza aplikace nebo nemusíme ručně zajišťovat, aby naše aplikace znova naběhla, pokud by náhodou spadla. Pokud navíc máte přehled o Hadoop ekosystému, můžete využít nějaké další programy na správu Samza aplikací.</p>
<p>Samza aplikace se může spustit na kterémkoliv počítači v Hadoop clusteru, což má i nepříjemné důsledky. Třeba nemá smysl dlouhodobě ukládat data na lokální disk – po pádu aplikace může YARN aplikaci spustit na úplně jiném počítači a Samza k těm souborům najednou nebude mít přístup.</p>
<h2 id="hello-world-v-samze">Hello World v Samze</h2>
<p>Samza je out-of-the-box propojena s Kafkou. To není úplně náhoda, protože Samzu vyvíjí LinkedIn, stejně jako Kafku. Nicméně existuje možnost, jak Samzu propojit i s jnými systémy. My ale využíváme Kafku, takže v dalších částech budu popisovat právě propojení s Kafkou.</p>
<p>V nastavení Samzy nejdříve určíme, jaké Kafka topicy chceme číst. My chceme číst dva topicy, takže to zapíšeme takto:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">task.inputs=kafka.bidresponses,kafka.bidrequests</span><br></pre></td></tr></table></figure>
<p>V kódu implementujeme rozhraní <a href="http://samza.apache.org/learn/documentation/0.7.0/api/javadocs/org/apache/samza/task/StreamTask.html" target="_blank" rel="external">StreamTask</a>, která má metodu <code>process</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@Override&#10;public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) throws Exception &#123;&#10;    System.out.println(envelope.getMessage());&#10;&#125;&#10;``` &#10;&#10;Poka&#382;d&#233;, kdy&#382; Samza p&#345;e&#269;te z topicu n&#283;jakou zpr&#225;vu, zavol&#225; metodu `process` a zpr&#225;vu ulo&#382;&#237; do argumentu `envelope` spolu s dal&#353;&#237;mi metadaty. Uk&#225;zka ned&#283;l&#225; nic jin&#233;ho, ne&#382; &#382;e danou zpr&#225;vu vyp&#237;&#353;e na standardn&#237; v&#253;stup. V nastaven&#237; si m&#367;&#382;eme zvolit, jak&#253; (de)serializ&#233;r bude pro zpr&#225;vy pou&#382;it:</span><br></pre></td></tr></table></figure>
<p>systems.kafka.samza.msg.serde=string <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#10;Toto nastaven&#237; znamen&#225;, &#382;e ka&#382;d&#225; zpr&#225;va, kter&#225; samoz&#345;ejm&#283; p&#345;ijde jen jako pole bajt&#367;, bude serializov&#225;na na Javov&#253; String. M&#367;&#382;ete si napsat vlastn&#237; (de)serializ&#233;r. &#10;&#10;Tohle jsou asi nejd&#367;le&#382;it&#283;j&#353;&#237; nastaven&#237; a nejd&#367;le&#382;it&#283;j&#353;&#237; funkce, kter&#233; v Samze jsou. Pokud si chcete Samzu opravdu vyzkou&#353;et, projd&#283;te si [Hello Samza](http://samza.apache.org/startup/hello-samza/0.8/). &#10;&#10;&#10;## Propojen&#237; s Kafkou&#10;&#10;Z&#225;kladn&#237;m stavebn&#237;m prvkem Samzy je n&#283;jak&#253; *StreamTask*, tedy t&#345;&#237;da, kter&#225; m&#225; metodu `process` a kter&#225; se star&#225; o zpracov&#225;v&#225;n&#237; zpr&#225;v, kter&#233; te&#269;ou do Samzy. Samza framework vytv&#225;&#345;&#237; instanci n&#283;jak&#233;ho StreamTasku na za&#269;&#225;tku b&#283;hu programu a tato instance p&#345;e&#382;&#237;v&#225; a&#382; do konce aplikace. &#10;&#10;Samza d&#225;le vytv&#225;&#345;&#237; instanci StreamTasku pro ka&#382;dou partition v Kafka topicu. M&#225;-li n&#225;&#353; topic 8 partition&#367;, vytvo&#345;&#237; se 8 instanc&#237; na&#353;eho konkr&#233;tn&#237;ho StreamTasku. Ka&#382;d&#253; StreamTask pak &#269;te svou vlastn&#237; partition. V&#353;echny instance pob&#283;&#382;&#237; v jednom vl&#225;kn&#283;, nemus&#237;me se tak b&#225;t n&#283;jak&#253;ch synchroniza&#269;n&#237;ch probl&#233;m&#367;. &#10;&#10;### YARN kontejnery&#10;&#10;Co ale d&#283;lat, kdy&#382; n&#225;&#353; Samza job nest&#237;h&#225; zpracov&#225;vat zpr&#225;vy, kter&#233; do Kafky pos&#237;l&#225;me? Proto&#382;e v&#353;echny instance StreamTasku b&#283;&#382;&#237; by design v jednom vl&#225;kn&#283;, nesta&#269;&#237; n&#225;m p&#345;idat CPU nebo n&#283;co podobn&#233;ho. Mus&#237;me zm&#283;nit po&#269;et kontejner&#367;, ve kter&#253;ch se Samza job spou&#353;t&#237;:</span><br></pre></td></tr></table></figure></p>
<p>yarn.container.count=4 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#10;Kontejner je pojem z YARNu/Hadoopu, pro n&#225;s aktu&#225;ln&#283; sta&#269;&#237;, kdy&#382; si p&#345;edstav&#237;me, &#382;e jeden kontejner = jeden Java proces = 1 vyu&#382;it&#233; CPU. V uk&#225;zce jsme nastavili &#269;ty&#345;i kontejnery, co&#382; znamen&#225;, &#382;e kdy&#382; spust&#237;me Samza job, vytvo&#345;&#237; se celkem &#269;ty&#345;i kontejnery / &#269;ty&#345;i Java procesy, p&#345;i&#269;em&#382; ka&#382;d&#253; m&#367;&#382;e zabrat a&#382; jedno cel&#233; CPU. V ka&#382;d&#233;m kontejneru se budou &#269;&#237;st pr&#225;v&#283; dv&#283; Kafka partitiony. Nem&#225; smysl nastavovat v&#237;ce ne&#382; osm kontejner&#367;, proto&#382;e jedna Kafka partition nem&#367;&#382;e b&#253;t &#269;tena ve dvou kontejnerech. &#10;&#10;To ale vyvol&#225;v&#225; jeden probl&#233;m: v jedn&#233; partition Kafky by m&#283;lo t&#233;ct maxim&#225;ln&#283; tolik zpr&#225;v, kolik jsme schopni zpracov&#225;vat na jednom CPU v Samze. Proto&#382;e pokud by v jedn&#233; partition teklo v&#237;ce zpr&#225;v, nebudeme nikdy schopni v Samze zpracov&#225;vat aktu&#225;ln&#237; data, budeme se neust&#225;le zpo&#382;&#271;ovat. Pokud nest&#237;h&#225;me zpracov&#225;vat topic *bidrequests*, m&#225;me n&#225;sleduj&#237;c&#237; mo&#382;nosti: &#10;&#10;- Napsat jin&#253;, jednodu&#353;&#353;&#237;, Samza job, kter&#253; bude &#269;&#237;st zpr&#225;vy z jedn&#233; partiton topicu *bidrequests* a bude je ukl&#225;dat do jin&#233;ho topicu, nap&#345;&#237;klad *bidrequests_splitted*, kter&#233;mu zv&#283;t&#353;&#237;me po&#269;et partiton. Budeme &#269;&#237;st zpr&#225;vy z partiton 0 topicu *bidrequests* a budeme je pos&#237;lat do partiton 0, 1, 2, 3 topic *bidrequests_splitted*. Zpr&#225;vy z partiton 1 budeme pos&#237;lat do 4, 5, 6, 7 atp. T&#237;m doc&#237;l&#237;me toho, &#382;e v jedn&#233; partiton topicu *bidrequests_splitted* bude &#269;tvrtina zpr&#225;v oproti jedn&#233; partiton topicu *bidrequests*. Pokud je v jedn&#233; partition tolik zpr&#225;v, &#382;e ani tato mo&#382;nost nep&#345;ipad&#225; v &#250;vahu, zb&#253;v&#225; prakticky jen n&#225;sleduj&#237;c&#237; mo&#382;nost: &#10;&#10;- Zv&#253;&#353;it po&#269;et partiton topicu *bidrequests* a upravit producery. &#10;&#10;Tak&#382;e ve stru&#269;nosti: volba `yarn.container.count` v&#237;ce m&#233;n&#283; nastavuje po&#269;et CPU, kter&#233; m&#367;&#382;e job se&#382;rat a toto &#269;&#237;slo nem&#367;&#382;e b&#253;t nikdy vy&#353;&#353;&#237; ne&#382; po&#269;et partiton vstupn&#237;ho Kafka kan&#225;lu. &#10;&#10;&#10;### V&#237;ce vstupn&#237;ch Kafka topic&#367;&#10;&#10;Jak se budou StreamTasky chovat, kdy&#382; m&#225;me na vstupu v&#237;ce Kafka topic&#367;? Jednodu&#353;e tak, &#382;e se vytvo&#345;&#237; tolik instanc&#237; StreamTask&#367;, jak&#253; je maxim&#225;ln&#237; po&#269;et partition&#367; ze v&#353;ech topic&#367;. &#268;teme-li &#269;ty&#345;i topicy, kter&#233; maj&#237; po&#269;et partition&#367; rovny 2, 5, 7, 6, tak se vytvo&#345;&#237; celkem 7 instanc&#237; StreamTask&#367;. Tohle moc smyslupln&#233; nen&#237;, obvykl&#233; je, &#382;e po&#269;et partition&#367; v&#353;ech topic&#367; je identick&#253;. &#10;&#10;Na&#353;e topicy *bidrequests* a *bidresponses* maj&#237; zat&#237;m 32 partition&#367; a spou&#353;t&#237;me je v osmi kontejnerech; ka&#382;d&#253; kontejner &#269;te &#269;ty&#345;i partitiony. Celkem se vytvo&#345;&#237; 32 instanc&#237; StreamTask&#367;. Ka&#382;d&#253; StreamTask &#269;te v&#382;dy stejnou partition nap&#345;&#237;&#269; v&#353;emi topicy. Nult&#253; StreamTask &#269;te data z partition 0 topicu *bidrequests* a z partition 0 topicu *bidresponses*. T&#345;et&#237; StreamTask &#269;te t&#345;et&#237; partition *bidrequests* i t&#345;et&#237; partition *bidresponses* atp. &#10;&#10;Jedn&#237;m z &#250;kol&#367;, kter&#233; n&#225;&#353; Samza job d&#283;l&#225; je, &#382;e p&#225;ruje po&#382;adavky a odpov&#283;di. V&#353;echny po&#382;adavky a odpov&#283;di, kter&#233; se t&#253;kaj&#237; jedn&#233; konkr&#233;tn&#237; imprese maj&#237; shodn&#233; *requestId*. Mus&#237;me b&#253;t schopni zajistit, aby po&#382;adavek s *requestId* 4742 &#353;el do stejn&#233; partition jako odpov&#283;&#271; s *requestId* 4742. Jinak je nikdy nesp&#225;rujeme, proto&#382;e pokud bude po&#382;adavek v partition 2 a odpov&#283;&#271; v partition 7, m&#367;&#382;ou je p&#345;e&#269;&#237;st dva r&#367;zn&#233; Samza joby, kter&#233; potenci&#225;ln&#283; ani neb&#283;&#382;&#237; na stejn&#233; ma&#353;in&#283; -- a i kdyby b&#283;&#382;ely, tak by se je p&#345;e&#269;etly dv&#283; r&#367;zn&#233; instance StreamTasku. &#10;&#10;Toho jsme doc&#237;lili tak, &#382;e &#269;&#237;slo partition, do kter&#233; se maj&#237; po&#382;adavky a odpov&#283;di pos&#237;lat, jsme vypo&#269;&#237;tali na z&#225;klad&#283; *requestId*, tj. asi takto:</span><br></pre></td></tr></table></figure></p>
<p>function getPartitionNumber(message, numberOfPartitions) { return murmurhash(message.requestId) % numberOfPartitions; } ```</p>
<p>Napsal jsem, že kód Samza jobu běží v jednom vlákně, takže by se zdálo, že nikdy nemůže zabírat více než jedno CPU. V realitě ale pozorujeme, že náš Samza job zabírá třeba 130 % CPU. Přičítáme to tomu, že náš kód sice v jednom vlákně běží, ale kromě našeho kódu je tam ještě kód frameworku, který už nejspíš běží v jiných vláknech. V těch pak probíhá čtení zpráv z Kafky nebo zápis do Kafky, takže v součtu může jeden Samza job zabrat více než 1 CPU.</p>
<h2 id="příště">Příště…</h2>
<p>Příště se podíváme na to, co je to ten windowing, jak lze využít metodu <code>window</code> a jaké jsou s tím spojeny problémy.</p>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/samza/#disqus_thread</comments>
    </item>
    
    <item>
      <title><![CDATA[Jak funguje replikace v Kafce]]></title>
      <link>http://programio.havrlant.cz/kafka-replikace/</link>
      <guid>http://programio.havrlant.cz/kafka-replikace/</guid>
      <pubDate>Mon, 30 Mar 2015 18:31:02 GMT</pubDate>
      <description>
      <![CDATA[<p>Co se stane, když nám server s Kafka Brokerem shoří? Není Kafka Broker single point of failure? Asi všichni tušíme, že není… Pro každý to]]>
      </description>
      <content:encoded><![CDATA[<p>Co se stane, když nám server s Kafka Brokerem shoří? Není Kafka Broker single point of failure? Asi všichni tušíme, že není… Pro každý topic totiž lze nastavit počet replikací. Pokud pro topic nastavíme replikaci na hodnotu tři, znamená to, že všechny zprávy, které do topicu pošleme, se zkopírují na další dva servery (respektive Kafka brokery) a jedna zpráva se bude celkem nacházet na třech serverech.</p>
<p>Důležité je, že se o replikaci nemusí starat producer – ten stále odesílá zprávy na <em>jeden</em> server a Kafka cluster už si sám řeší replikaci dat na další servery. Flow vypadá takto:</p>
<div class="figure">
<img src="/images/kafka/kafkamutliplebrokersbasic.svg" alt="">

</div>
<p>Producer pošle zprávu brokerovi #1, ten zprávu zapíše k sobě na disk a zároveň se zpráva replikuje u brokerů #2 a #3, které si ji také zapíší na disk. Tím dojde k záloze a pokud Broker #1 vypadne, provoz se automaticky přesměruje na jeden ze zbývajících brokerů:</p>
<div class="figure">
<img src="/images/kafka/kafkamutliplebrokerschanged.svg" alt="">

</div>
<p>Replikací nicméně nezvýšíme propustnost služby. Máme-li nastavenou replikaci 3, neznamená to, že když budeme číst zprávy z tohoto topicu, že budeme číst zprávy paralelně ze všech tří brokerů. Vždy se zvolí jeden broker jako leader, z něj se čtou zprávy a zbylé dva servery se pro čtení nepoužijí. V předchozím obrázku například consumer čte zprávy z brokeru #2 a z brokeru #3 žádné zprávy nedostává, přestože tento broker má zprávy z daného topicu uložené u sebe na disku.</p>
<h2 id="isr">ISR</h2>
<p>Teď trochu detailněji o tom, jak replikce funguje. Tímto příkazem založíme topic:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper zookeeper01:<span class="number">2181</span> \</span><br><span class="line">  --replication-factor <span class="number">3</span> --partitions <span class="number">2</span> --topic bidrequests</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">Kafka musí založit dvě partitions, pro každou partition musí vybrat celkem tři brokery, které budou tyto partitiony obsluhovat a replikovat. Dejme tomu, že pro partitionu <span class="number">0</span> zvolí brokery kafka00, kafka01 a kafka02 a pro partition <span class="number">1</span> kafka03, kafka04 a kafka05. V každé skupině se zvolí leader pro danou partition. Leaderem se tak mohou stát například stroje kafka01 a kafka05. Ostatní Kafky slouží v tuto chvíli jako repliky. </span><br><span class="line"></span><br><span class="line">Kafka dále definuje pojem *<span class="keyword">in</span> sync replica*, což je broker, který je <span class="string">"živý"</span> (= udržuje svou session se ZooKeeperem) a který stíhá dostatečně rychle replikovat zprávy z leaderu (= offset poslední replikované zprávy je maximálně o x zpráv pozadu oproti leaderovi). </span><br><span class="line"></span><br><span class="line">Kafka si neustále udržuje informace o tom, které repliky jsou <span class="string">"aktuální"</span>. Může se totiž stát, že replikace spadne nebo že replikace přestane stíhat stahovat a ukládat data. Ty servery, které to stíhají, jsou nazývány *<span class="keyword">in</span> sync replica*, zkráceně ISR. Kafka si pak pro každou partitionu udržuje v ZooKeeperu množinu ISR, můžete se na ně podívat v ZooKeeper konzoli (`bin/zkCli.sh`):</span><br><span class="line"></span><br><span class="line">```plain</span><br><span class="line">$ get /brokers/topics/bidrequests/partitions/<span class="number">0</span>/state</span><br><span class="line">&#123;<span class="string">"leader"</span>:<span class="number">1</span>, <span class="string">"version"</span>:<span class="number">1</span>, <span class="string">"isr"</span>:[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]&#125;</span><br><span class="line"></span><br><span class="line">$ get /brokers/topics/bidrequests/partitions/<span class="number">0</span>/state</span><br><span class="line">&#123;<span class="string">"leader"</span>:<span class="number">5</span>, <span class="string">"version"</span>:<span class="number">1</span>, <span class="string">"isr"</span>:[<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">Záznam v ZooKeeperu říká, že leaderem partitiony <span class="number">0</span> topicu *bidrequests* je kafka01 a synchronizované repliky jsou kafka02, kafka01 a kafka00 (i samotný leader je ISR). Tyto repliky pravidelně stahují nové zprávy z leadera a replikují je k sobě. Leader poskytne zprávu ke konzumaci pouze ve chvíli, kdy všechny ISR potvrdí, že zprávu přijaly.</span><br><span class="line"></span><br><span class="line">Pokud ISR spadne nebo pokud nestíhá stahovat nové zprávy z leadera, je tento uzel odstraněn z množiny ISR v ZooKeeperu a pokračuje se dále. Dejme tomu, že by kafka00 kompletně spadla:</span><br><span class="line"></span><br><span class="line">```plain</span><br><span class="line">$ get /brokers/topics/bidrequests/partitions/<span class="number">0</span>/state</span><br><span class="line">&#123;<span class="string">"leader"</span>:<span class="number">1</span>, <span class="string">"version"</span>:<span class="number">1</span>, <span class="string">"isr"</span>:[<span class="number">2</span>,<span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>Leader by od této chvíle čekal na potvrzení jen od kafka02. Pokud by spadnul leader, stala by se leaderem kafka02:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ get /brokers/topics/bidrequests/partitions/0/state&#10;&#123;&#34;leader&#34;:2, &#34;version&#34;:1, &#34;isr&#34;:[2]&#125;</span><br></pre></td></tr></table></figure>
<p>Všechny zprávy by v tuto chvíli tekly jen přes kafka02 a docházelo by k nulové záloze. Je přitom jedno, že vedle máme stroje kafka03, kafka04 a kafka05, které žijí – partition 0 má nastaveno, že poběží na strojích kafka00, kafka01 a kafka02, takže dokud tyto stroje neoživíme, nebude docházet k žádné replikaci. Nemusí to být samozřejmě <em>stejné</em> servery, stačí spustit Kafka broker se stejným ID.</p>
<p>Pokud jakýmkoliv způsobem přivedeme k životu kafka01, ta zjistí, že by měla obsluhovat partition 0 topicu bidrequests, takže si začne z leaderu tahat chybějící data. Až je natáhne, stane se zase ISR, což se uloží do ZooKeeperu. Leader se nezmění.</p>
<pre class="plain"><code>$ get /brokers/topics/bidrequests/partitions/0/state
{&quot;leader&quot;:2, &quot;version&quot;:1, &quot;isr&quot;:[2,1]}</code></pre>
<p>Máme-li nastavenou replikaci <code>r</code>, náš systém bude tolerovat až <code>r-1</code> nedostupných ISR. Pokud vypadne všech <code>r</code> strojů, máme samozřejmě smůlu.</p>
<h2 id="nastavení-kafky-týkající-se-replikace">Nastavení Kafky týkající se replikace</h2>
<p>Kafka umožňuje více specifikovat, jak se v určitých situacích týkajících se replikace chovat:</p>
<ul>
<li><code>request.required.acks</code> určuje, jestli bude producer čekat na potvrzení doručení zprávy. Možné hodnoty:
<ul>
<li><code>0</code>: Producer nečeká na žádné potvrzení, rychlost odesílání je nejvyšší.</li>
<li><code>1</code>: Producer čeká na potvrzení leadera.</li>
<li><code>-1</code>: Producer čeká na potvrzení leadera a všech replik v ISR. Při tomto nastavení bue producer nejpomalejší, protože bude relativně dlouho čekat na potvrzení doručení zpráv, ale zase máte největší jistotu, že o žádnou zprávu nepřijdete.</li>
</ul></li>
<li><code>min.insync.replicas</code> určuje minimální počet ISR. Pokud například nastavíte <code>request.required.acks</code> na <code>-1</code>, tj. producer bude čekat na potvrzení od všech ISR, může se stejně stát, že všechny replicy spadnou a ISR bude obsahovat pouze leadera. Tímto nastavením můžete říci, že pokud je velikost ISR menší než <code>min.insync.replicas</code>, začne producer vyhazovat při odesílání výjimky.</li>
</ul>
]]></content:encoded>
      <comments>http://programio.havrlant.cz/kafka-replikace/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
